{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXdeHDmEPQM6"
   },
   "source": [
    "## G-learning for wealth optimization\n",
    "\n",
    "The purpose of this notebook is to demonstrate the use of G-learning with quadratic rewards for optimization of a defined contribution retirement plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOoE16hWPQNC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PHyhUTpJ4nl1",
    "outputId": "18b1c78b-d6c6-4453-9755-65390364ab53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KMP_DUPLICATE_LIB_OK=TRUE\n"
     ]
    }
   ],
   "source": [
    "%env KMP_DUPLICATE_LIB_OK=TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0yKeMU9PQNa"
   },
   "outputs": [],
   "source": [
    "# set the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLivr9yuPQON"
   },
   "outputs": [],
   "source": [
    "# Define the G-learning portfolio optimization class\n",
    "class G_learning_portfolio_opt:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_steps,\n",
    "                 params,\n",
    "                 beta,\n",
    "                 benchmark_portf,\n",
    "                 gamma, \n",
    "                 num_risky_assets,\n",
    "                 riskfree_rate,\n",
    "                 exp_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns of risky assets\n",
    "                 init_x_vals, # array of initial asset position values (num_risky_assets + 1)\n",
    "                 use_for_WM = True): # use for wealth management tasks\n",
    "\n",
    "                \n",
    "        self.num_steps = num_steps\n",
    "        self.num_assets = num_risky_assets + 1 \n",
    "        \n",
    "        self.lambd = torch.tensor(params[0], requires_grad=False, dtype=torch.float64)\n",
    "        self.Omega_mat = params[1] * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "        self.eta = torch.tensor(params[2], requires_grad=False, dtype=torch.float64)\n",
    "        self.rho = torch.tensor(params[3], requires_grad=False, dtype=torch.float64)\n",
    "        self.beta = torch.tensor(beta, requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.use_for_WM = use_for_WM\n",
    "        \n",
    "        self.num_risky_assets = num_risky_assets\n",
    "        self.r_f = riskfree_rate\n",
    "        \n",
    "        \n",
    "        assert exp_returns.shape[0] == self.num_steps\n",
    "        assert Sigma_r.shape[0] == Sigma_r.shape[1]\n",
    "        assert Sigma_r.shape[0] == num_risky_assets # self.num_assets\n",
    "        \n",
    "        self.Sigma_r_np = Sigma_r # array of shape num_stocks x num_stocks\n",
    "        \n",
    "        self.reg_mat = 1e-3*torch.eye(self.num_assets, dtype=torch.float64)\n",
    "        \n",
    "        # arrays of returns for all assets including the risk-free asset\n",
    "        # array of shape num_steps x (num_stocks + 1) \n",
    "        self.exp_returns_np = np.hstack((self.r_f * np.ones(self.num_steps).reshape((-1,1)), exp_returns))\n",
    "                                      \n",
    "        # make block-matrix Sigma_r_tilde with Sigma_r_tilde[0,0] = 0, and equity correlation matrix inside\n",
    "        self.Sigma_r_tilde_np = np.zeros((self.num_assets, self.num_assets))\n",
    "        self.Sigma_r_tilde_np[1:,1:] = self.Sigma_r_np\n",
    "            \n",
    "        # make Torch tensors  \n",
    "        self.exp_returns = torch.tensor(self.exp_returns_np,requires_grad=False, dtype=torch.float64)\n",
    "        self.Sigma_r = torch.tensor(Sigma_r,requires_grad=False, dtype=torch.float64)\n",
    "        self.Sigma_r_tilde = torch.tensor(self.Sigma_r_tilde_np,requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        self.benchmark_portf = torch.tensor(benchmark_portf, requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        # asset holding values for all times. Initialize with initial values, \n",
    "        # values for the future times will be expected values \n",
    "        self.x_vals_np = np.zeros((self.num_steps, self.num_assets))\n",
    "        self.x_vals_np[0,:] = init_x_vals \n",
    "        \n",
    "        # Torch tensor\n",
    "        self.x_vals = torch.tensor(self.x_vals_np)\n",
    "                \n",
    "        # allocate memory for coefficients of R-, F- and G-functions        \n",
    "        self.F_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets, dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.F_x = torch.zeros(self.num_steps, self.num_assets, dtype=torch.float64,\n",
    "                               requires_grad=True)\n",
    "        self.F_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        \n",
    "        self.Q_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_uu = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_ux = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_x = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.Q_u = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.Q_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        \n",
    "        self.R_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_uu = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_ux = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_x = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.R_u = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.R_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "\n",
    "        \n",
    "        self.reset_prior_policy()\n",
    "        \n",
    "        # the list of adjustable model parameters:\n",
    "        self.model_params = [self.lambd, self.beta, self.Omega_mat, self.eta]         \n",
    "        \n",
    "        # expected cash installment for all steps\n",
    "        self.expected_c_t = torch.zeros(self.num_steps,dtype=torch.float64)\n",
    "        \n",
    "        # realized values of the target portfolio\n",
    "        self.realized_target_portf = np.zeros(self.num_steps,dtype=np.float64)\n",
    "        \n",
    "        # expected portfolio values for all times\n",
    "        self.expected_portf_val = torch.zeros(self.num_steps,dtype=torch.float64)\n",
    "        \n",
    "        # the first value is the sum of initial position values\n",
    "        self.expected_portf_val[0] = self.x_vals[0,:].sum()\n",
    "\n",
    "    def reset_prior_policy(self):\n",
    "        # initialize time-dependent parameters of prior policy \n",
    "        self.u_bar_prior = torch.zeros(self.num_steps,self.num_assets,requires_grad=False,\n",
    "                                       dtype=torch.float64)\n",
    "        self.v_bar_prior =  torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        self.Sigma_prior =  torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        self.Sigma_prior_inv = torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        \n",
    "        # make each time elements of v_bar_prior and Sigma_prior proportional to the unit matrix\n",
    "        for t in range(self.num_steps):\n",
    "            self.v_bar_prior[t,:,:] = 0.1 * torch.eye(self.num_assets).clone()\n",
    "            self.Sigma_prior[t,:,:] = 0.1 * torch.eye(self.num_assets).clone()\n",
    "            self.Sigma_prior_inv[t,:,:] = 10.0 * torch.eye(self.num_assets).clone() # np.linalg.inv(self.Sigma_prior[t,:,:])\n",
    "    \n",
    "    def reward_fun(self, t, x_vals, u_vals, exp_rets, lambd, Sigma_hat):\n",
    "        \"\"\"\n",
    "        The reward function \n",
    "        \"\"\"\n",
    "        x_plus = x_vals + u_vals\n",
    "        \n",
    "        p_hat = self.rho.clone() * self.benchmark_portf[t] + (1-self.rho.clone())*self.eta.clone()*x_vals.sum()\n",
    "        \n",
    "        aux_1 = - self.lambd.clone() * p_hat**2         \n",
    "        aux_2 = - u_vals.sum()   \n",
    "        aux_3 = 2*self.lambd.clone() * p_hat * x_plus.dot(torch.ones(num_assets) + exp_rets)\n",
    "        aux_4 = - self.lambd.clone() * x_plus.mm(Sigma_hat.mv(x_plus))\n",
    "        aux_5 = - u_vals.mm(self.Omega_mat.clone().mv(u_vals))\n",
    "        \n",
    "        return aux_1 + aux_2 + aux_3 + aux_4 + aux_5  \n",
    "    \n",
    "    def compute_reward_fun(self):\n",
    "        \"\"\"\n",
    "        Compute coefficients R_xx, R_ux, etc. for all steps\n",
    "        \"\"\"\n",
    "        for t in range(0, self.num_steps):\n",
    "            \n",
    "            one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "            benchmark_portf = self.benchmark_portf[t]\n",
    "            Sigma_hat = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "            \n",
    "            one_plus_exp_ret_by_one = torch.ger(one_plus_exp_ret,torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            one_plus_exp_ret_by_one_T = one_plus_exp_ret_by_one.t()     \n",
    "            one_one_T_mat = torch.ones(self.num_assets,self.num_assets)\n",
    "            \n",
    "            self.R_xx[t,:,:] = (-self.lambd.clone()*(self.eta.clone()**2)*(self.rho.clone()**2)*one_one_T_mat\n",
    "                                 + 2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*one_plus_exp_ret_by_one\n",
    "                                 - self.lambd.clone()*Sigma_hat)\n",
    "            \n",
    "            self.R_ux[t,:,:] = (2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*one_plus_exp_ret_by_one\n",
    "                                 - 2*self.lambd.clone()*Sigma_hat)\n",
    "            \n",
    "            self.R_uu[t,:,:] = - self.lambd.clone() * Sigma_hat - self.Omega_mat.clone()\n",
    "            \n",
    "            self.R_x[t,:] =  (-2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*(1-self.rho.clone())*benchmark_portf *\n",
    "                                 torch.ones(self.num_assets,dtype=torch.float64)\n",
    "                                 + 2*self.lambd.clone()*(1-self.rho.clone())*benchmark_portf * one_plus_exp_ret)\n",
    "            \n",
    "            self.R_u[t,:] = (2*self.lambd.clone()*(1-self.rho.clone())*benchmark_portf * one_plus_exp_ret\n",
    "                             - torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            \n",
    "            self.R_0[t] = - self.lambd.clone()*((1-self.rho.clone())**2) * (benchmark_portf**2)\n",
    "                \n",
    "         \n",
    "    def project_cash_injections(self):\n",
    "        \"\"\"\n",
    "        Compute the expected values of future asset positions, and the expected cash injection for future steps,\n",
    "        as well as realized values of the target portfolio\n",
    "        \"\"\"\n",
    "           \n",
    "        # this assumes that the policy is trained\n",
    "        for t in range(1, self.num_steps):  # the initial value is fixed \n",
    "            # increment the previous x_t\n",
    "            delta_x_t = self.u_bar_prior[t,:] + self.v_bar_prior[t,:,:].mv(self.x_vals[t-1,:])\n",
    "            self.x_vals[t,:] = self.x_vals[t-1,:] + delta_x_t\n",
    "            \n",
    "            # grow using the expected return\n",
    "            self.x_vals[t,:] = (torch.ones(self.num_assets)+ self.exp_returns[t,:])*self.x_vals[t,:]\n",
    "            \n",
    "            # compute c_t\n",
    "            self.expected_c_t[t] = delta_x_t.sum().data # detach().numpy()\n",
    "            \n",
    "            # expected portfolio value for this step\n",
    "            self.expected_portf_val[t] = self.x_vals[t,:].sum().data # .detach().numpy()\n",
    "                                                                                      \n",
    "    def set_terminal_conditions(self):\n",
    "        \"\"\"\n",
    "        set the terminal condition for the F-function\n",
    "        \"\"\"\n",
    "        \n",
    "        # the auxiliary quantity to perform matrix calculations\n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[-1,:]\n",
    "        \n",
    "        # Compute the reward function for all steps (only the last step is needed for this functions, while \n",
    "        # values for other time steps will be used in other functions)\n",
    "        self.compute_reward_fun()\n",
    "        \n",
    "        if self.use_for_WM:\n",
    "            Sigma_hat = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "            Sigma_hat_inv = torch.inverse(Sigma_hat + self.reg_mat)\n",
    "            \n",
    "            Sigma_tilde = Sigma_hat + (1/self.lambd)*self.Omega_mat.clone()\n",
    "            Sigma_tilde_inv = torch.inverse(Sigma_tilde + self.reg_mat)\n",
    "            \n",
    "            Sigma_hat_sigma_tilde = Sigma_hat.mm(Sigma_tilde)\n",
    "            Sigma_tilde_inv_sig_hat = Sigma_tilde_inv.mm(Sigma_hat)\n",
    "            Sigma_tilde_sigma_hat = Sigma_tilde.mm(Sigma_hat)\n",
    "            \n",
    "            Sigma_hat_Sigma_tilde_inv = Sigma_hat.mm(Sigma_tilde_inv)\n",
    "            Sigma_3_plus_omega = self.lambd*Sigma_tilde_inv.mm(Sigma_hat_Sigma_tilde_inv) + self.Omega_mat.clone()    \n",
    "                             \n",
    "            one_plus_exp_ret_by_one = torch.ger(one_plus_exp_ret,torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            one_plus_exp_ret_by_one_T = one_plus_exp_ret_by_one.t()     \n",
    "            one_one_T_mat = torch.ones(self.num_assets,self.num_assets)\n",
    "            \n",
    "            Sigma_tilde_inv_t_R_ux = Sigma_tilde_inv.t().mm(self.R_ux[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_uu = Sigma_tilde_inv.t().mm(self.R_uu[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_u = Sigma_tilde_inv.t().mv(self.R_u[-1,:].clone())\n",
    "            \n",
    "            Sigma_tilde_inv_R_u = Sigma_tilde_inv.mv(self.R_u[-1,:].clone())\n",
    "            Sigma_tilde_inv_R_ux = Sigma_tilde_inv.mm(self.R_ux[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_uu = Sigma_tilde_inv.mm(self.R_uu[-1,:,:].clone())\n",
    "            \n",
    "            # though the action at the last step is deterministic, we can feed \n",
    "            # parameters of the prior with these values                     \n",
    "              \n",
    "            self.u_bar_prior[-1,:]   = (1/(2 * self.lambd.clone()))* Sigma_tilde_inv.clone().mv(self.R_u[-1,:].clone())\n",
    "            self.v_bar_prior[-1,:,:] = (1/(2 * self.lambd.clone()))* Sigma_tilde_inv.clone().mm(self.R_ux[-1,:,:].clone())    \n",
    "                \n",
    "            # First compute the coefficients of the reward function F at the last step:        \n",
    "            # F_xx                 \n",
    "            self.F_xx[-1,:,:] = (self.R_xx[-1,:,:].clone()\n",
    "                                 + (1/(2*self.lambd.clone()))* self.R_ux[-1,:,:].clone().t().mm(Sigma_tilde_inv_t_R_ux)\n",
    "                                 + (1/(4*self.lambd.clone()**2))* self.R_ux[-1,:,:].clone().t().mm(\n",
    "                                      Sigma_tilde_inv_t_R_uu.clone().mm(Sigma_tilde_inv.clone().mm(self.R_ux[-1,:,:].clone())))\n",
    "                                )\n",
    "            \n",
    "            # F_x                    \n",
    "            self.F_x[-1,:] = (self.R_x[-1,:].clone()\n",
    "                                 + (1/(self.lambd.clone()))* self.R_ux[-1,:,:].clone().t().mv(Sigma_tilde_inv_t_R_u.clone())\n",
    "                                 + (1/(2*self.lambd.clone()**2))* self.R_ux[-1,:,:].clone().t().mv(\n",
    "                                      Sigma_tilde_inv_t_R_uu.clone().mv(Sigma_tilde_inv_R_u.clone()))\n",
    "                            )\n",
    "            \n",
    "            # F_0   \n",
    "            self.F_0[-1] = (self.R_0[-1].clone() \n",
    "                            +  (1/(2*self.lambd.clone()))* self.R_u[-1,:].clone().dot(Sigma_tilde_inv_R_u.clone())\n",
    "                            + (1/(4*self.lambd.clone()**2))* self.R_u[-1,:].clone().dot(\n",
    "                                Sigma_tilde_inv_t_R_uu.clone().mv(Sigma_tilde_inv_R_u.clone()))\n",
    "                           )\n",
    "            \n",
    "            # for the Q-function at the last step:\n",
    "            self.Q_xx[-1,:,:] = self.R_xx[-1,:,:].clone()\n",
    "            self.Q_ux[-1,:,:] = self.R_ux[-1,:,:].clone()\n",
    "            self.Q_uu[-1,:,:] = self.R_uu[-1,:,:].clone()\n",
    "            self.Q_u[-1,:] = self.R_u[-1,:].clone()\n",
    "            self.Q_x[-1,:] = self.R_x[-1,:].clone()\n",
    "            self.Q_0[-1] = self.R_0[-1].clone()\n",
    "            \n",
    "    def G_learning(self, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        find the optimal policy for the time dependent policy\n",
    "        \n",
    "        \"\"\"   \n",
    "        print('Doing G-learning, it may take a few seconds...')\n",
    "        \n",
    "        # set terminal conditions\n",
    "        self.set_terminal_conditions()\n",
    "        \n",
    "        # allocate iteration numbers for all steps\n",
    "        self.iter_counts = np.zeros(self.num_steps)\n",
    "        \n",
    "        # iterate over time steps backward\n",
    "        for t in range(self.num_steps-2,-1,-1):\n",
    "            self.step_G_learning(t, err_tol, max_iter)\n",
    "            \n",
    "    def step_G_learning(self, t, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        Perform one step of backward iteration for G-learning self-consistent equations\n",
    "        This should start from step t = num_steps - 2 (i.e. from a step that is before the last one)\n",
    "        \"\"\"\n",
    "            \n",
    "        # make matrix Sigma_hat_t        \n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "        Sigma_hat_t = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "        \n",
    "        # matrix A_t = diag(1 + r_bar_t)\n",
    "        A_t = torch.diag(torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:])\n",
    "                    \n",
    "        # update parameters of Q_function using next-step F-function values\n",
    "        self.update_Q_params(t, A_t,Sigma_hat_t)\n",
    "             \n",
    "        # iterate between policy evaluation and policy improvement  \n",
    "        while self.iter_counts[t] < max_iter:\n",
    "                \n",
    "            curr_u_bar_prior = self.u_bar_prior[t,:].clone()  \n",
    "            curr_v_bar_prior = self.v_bar_prior[t,:,:].clone()     \n",
    "                \n",
    "            # compute parameters of F-function for this step from parameters of Q-function\n",
    "            self.update_F_params(t) \n",
    "              \n",
    "            # Policy iteration step: update parameters of the prior policy distribution\n",
    "            # with given Q- and F-function parameters\n",
    "            self.update_policy_params(t)    \n",
    "            \n",
    "            # difference between the current value of u_bar_prior and the previous one\n",
    "            err_u_bar = torch.sum((curr_u_bar_prior - self.u_bar_prior[t,:])**2)\n",
    "            \n",
    "            # divide by num_assets in err_v_bar to get both errors on a comparable scale\n",
    "            err_v_bar = (1/self.num_assets)*torch.sum((curr_v_bar_prior - self.v_bar_prior[t,:,:])**2)\n",
    "            \n",
    "            # choose the difference from the previous iteration as the maximum of the two errors\n",
    "            tol = torch.max(err_u_bar, err_v_bar)  # tol = 0.5*(err_u_bar + err_v_bar)\n",
    "            \n",
    "            self.iter_counts[t] += 1\n",
    "            # Repeat the calculation of Q- and F-values\n",
    "            if tol <= err_tol:\n",
    "                break\n",
    "                \n",
    "    def update_Q_params(self,t, A_t,Sigma_hat_t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of Q-function from (t+1)-parameters of F-function\n",
    "        \"\"\" \n",
    "                \n",
    "        ones = torch.ones(self.num_assets,dtype=torch.float64)    \n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "    \n",
    "        self.Q_xx[t,:,:] = (self.R_xx[t,:,:].clone() \n",
    "                            + self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() ) )\n",
    "\n",
    "\n",
    "        self.Q_ux[t,:,:] = (self.R_ux[t,:,:].clone() \n",
    "                            + 2 * self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() ) \n",
    "                           )\n",
    "    \n",
    "        self.Q_uu[t,:,:] = (self.R_uu[t,:,:].clone()  \n",
    "                            + self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() )\n",
    "                            - self.Omega_mat.clone()\n",
    "                           )\n",
    "\n",
    "\n",
    "        self.Q_x[t,:] = self.R_x[t,:].clone() + self.gamma * A_t.clone().mv(self.F_x[t+1,:].clone()) \n",
    "        self.Q_u[t,:] = self.R_u[t,:].clone() + self.gamma * A_t.clone().mv(self.F_x[t+1,:].clone())\n",
    "        self.Q_0[t]   = self.R_0[t].clone() + self.gamma * self.F_0[t+1].clone()\n",
    "\n",
    "    def update_F_params(self,t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of F-function from t-parameters of G-function\n",
    "        This is a policy evaluation step: it uses the current estimations of the mean parameters of the policy\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # produce auxiliary parameters U_t, W_t, Sigma_tilde_t\n",
    "        U_t = (self.beta.clone() * self.Q_ux[t,:,:].clone() \n",
    "               + self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone()))\n",
    "        W_t = (self.beta.clone() * self.Q_u[t,:].clone() \n",
    "               +  self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:]).clone())\n",
    "        Sigma_p_bar =  self.Sigma_prior_inv[t,:,:].clone() - 2 * self.beta.clone() * self.Q_uu[t,:,:].clone()\n",
    "        Sigma_p_bar_inv = torch.inverse(Sigma_p_bar + self.reg_mat)\n",
    "        \n",
    "        # update parameters of F-function\n",
    "        self.F_xx[t,:,:] = self.Q_xx[t,:,:].clone() + (1/(2*self.beta.clone()))*(U_t.t().mm(Sigma_p_bar_inv.clone().mm(U_t))\n",
    "                                    - self.v_bar_prior[t,:,:].clone().t().mm(\n",
    "                                        self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone())))\n",
    "        \n",
    "        \n",
    "        self.F_x[t,:] = self.Q_x[t,:].clone() + (1/self.beta.clone())*(U_t.mv(Sigma_p_bar_inv.clone().mv(W_t))\n",
    "                                    - self.v_bar_prior[t,:,:].clone().mv(\n",
    "                                        self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())))\n",
    "        \n",
    "        \n",
    "        self.F_0[t] = self.Q_0[t].clone() + ( (1/(2*self.beta.clone()))*(W_t.dot(Sigma_p_bar_inv.clone().mv(W_t))\n",
    "                                    - self.u_bar_prior[t,:].clone().dot(\n",
    "                                        self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())))\n",
    "                                    - (1/(2*self.beta.clone())) * (torch.log(torch.det(self.Sigma_prior[t,:,:].clone()+\n",
    "                                                                              self.reg_mat))\n",
    "                                                       - torch.log(torch.det(Sigma_p_bar_inv.clone() + self.reg_mat))) )\n",
    "\n",
    "    def update_policy_params(self,t):\n",
    "        \"\"\"\n",
    "        update parameters of the Gaussian policy using current coefficients of the F- and G-functions\n",
    "        \"\"\"\n",
    "        \n",
    "        new_Sigma_prior_inv = self.Sigma_prior_inv[t,:,:].clone() - 2 * self.beta.clone() * self.Q_uu[t,:,:].clone()\n",
    "\n",
    "        Sigma_prior_new = torch.inverse(new_Sigma_prior_inv + self.reg_mat)\n",
    "        \n",
    "        # update parameters using the previous value of Sigma_prior_inv\n",
    "        self.u_bar_prior[t,:] = Sigma_prior_new.mv(self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())\n",
    "                                              + self.beta.clone() * self.Q_u[t,:].clone())\n",
    "        \n",
    "        \n",
    "        self.v_bar_prior[t,:,:] = Sigma_prior_new.clone().mm(self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone())\n",
    "                                              + self.beta.clone() * self.Q_ux[t,:,:].clone())\n",
    "        \n",
    "        # and then assign the new inverse covariance for the prior for the next iteration\n",
    "        self.Sigma_prior[t,:,:] = Sigma_prior_new.clone()\n",
    "        self.Sigma_prior_inv[t,:,:] = new_Sigma_prior_inv.clone()\n",
    "        \n",
    "        # also assign the same values for the previous time step\n",
    "        if t > 0:\n",
    "            self.Sigma_prior[t-1,:,:] = self.Sigma_prior[t,:,:].clone()\n",
    "            self.u_bar_prior[t-1,:] = self.u_bar_prior[t,:].clone()\n",
    "            self.v_bar_prior[t-1,:,:] = self.v_bar_prior[t,:,:].clone()\n",
    "            \n",
    "    def trajs_to_torch_tensors(self,trajs):\n",
    "        \"\"\"\n",
    "        Convert data from a list of lists into Torch tensors\n",
    "        \"\"\"\n",
    "        num_trajs = len(trajs)\n",
    "        \n",
    "        self.data_xvals = torch.zeros(num_trajs,self.num_steps,self.num_assets,dtype=torch.float64)\n",
    "        self.data_uvals = torch.zeros(num_trajs,self.num_steps,self.num_assets,dtype=torch.float64)\n",
    "            \n",
    "        for n in range(num_trajs):\n",
    "            for t in range(self.num_steps):\n",
    "                self.data_xvals[n,t,:] = torch.tensor(trajs[n][t][0],dtype=torch.float64).clone()\n",
    "                self.data_uvals[n,t,:] = torch.tensor(trajs[n][t][1],dtype=torch.float64).clone()\n",
    "                \n",
    "    def compute_reward_on_traj(self,\n",
    "                              t,\n",
    "                              x_t, u_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.R_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_ux = u_t.dot(self.R_ux[t,:,:].clone().mv(x_t))\n",
    "        aux_uu = u_t.dot(self.R_uu[t,:,:].clone().mv(u_t))\n",
    "        aux_x = x_t.dot(self.R_x[t,:].clone())\n",
    "        aux_u = u_t.dot(self.R_u[t,:].clone())\n",
    "        aux_0 = self.R_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_ux + aux_uu + aux_x + aux_u + aux_0\n",
    "    \n",
    "    def compute_G_fun_on_traj(self,\n",
    "                              t,\n",
    "                              x_t, u_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.Q_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_ux = u_t.dot(self.Q_ux[t,:,:].clone().mv(x_t))\n",
    "        aux_uu = u_t.dot(self.Q_uu[t,:,:].clone().mv(u_t))\n",
    "        aux_x = x_t.dot(self.Q_x[t,:].clone())\n",
    "        aux_u = u_t.dot(self.Q_u[t,:].clone())\n",
    "        aux_0 = self.Q_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_ux + aux_uu + aux_x + aux_u + aux_0\n",
    "    \n",
    "    def compute_F_fun_on_traj(self,\n",
    "                              t,\n",
    "                              x_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.F_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_x = x_t.dot(self.F_x[t,:].clone())\n",
    "        aux_0 = self.F_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_x + aux_0\n",
    "                 \n",
    "    def MaxEntIRL(self,\n",
    "                  trajs,\n",
    "                  learning_rate,\n",
    "                  err_tol, max_iter):\n",
    "        \n",
    "        \"\"\"\n",
    "        Estimate parameters of the reward function using MaxEnt IRL.\n",
    "        Inputs:\n",
    "        \n",
    "        trajs - a list of trajectories. Each trajectory is a list of state-action pairs, stored as a tuple.\n",
    "                We assume each trajectory has the same length\n",
    "        \"\"\"\n",
    "        \n",
    "        # omega is a tunable parameter that determines the cost matrix self.Omega_mat\n",
    "        omega_init = 15.0\n",
    "        self.omega = torch.tensor(omega_init, requires_grad=True, dtype=torch.float64)\n",
    "        \n",
    "        beta_init = 50 # Beta is fixed and not a learned parameter.\n",
    "        self.beta = torch.tensor(beta_init, requires_grad=True, dtype=torch.float64)\n",
    "        \n",
    "        reward_params =  [self.lambd, self.eta, self.rho, self.omega, self.beta]\n",
    "        \n",
    "        print(\"Omega mat...\")\n",
    "        self.Omega_mat = self.omega * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "        print(\"g learning...\")\n",
    "        self.reset_prior_policy()\n",
    "        self.G_learning(err_tol, max_iter)\n",
    "        print(\"intialize optimizer...\")\n",
    "        optimizer = optim.Adam(reward_params, lr=learning_rate)\n",
    "        print(\"zero grad...\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        num_trajs = len(trajs)\n",
    "        print(\"trajs_to_torch_tensors...\")\n",
    "        \n",
    "        # fill in Torch tensors for the trajectory data\n",
    "        self.trajs_to_torch_tensors(trajs)\n",
    "        print(\"constructing zero tensors...\")   \n",
    "        self.realized_rewards = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        self.realized_cum_rewards = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=True)\n",
    "        print(\"constructing zero tensors...\")  \n",
    "        self.realized_G_fun = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64, requires_grad=True)\n",
    "        self.realized_F_fun = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64, requires_grad=True)\n",
    "        print(\"constructing zero tensors...\")  \n",
    "        self.realized_G_fun_cum = torch.zeros(num_trajs,dtype=torch.float64, requires_grad=True)\n",
    "        self.realized_F_fun_cum = torch.zeros(num_trajs,dtype=torch.float64, requires_grad=True)\n",
    "        print(\"done...\")  \n",
    "        \n",
    "        num_iter_IRL = 3\n",
    "        \n",
    "        for i in range(num_iter_IRL):\n",
    "            print('GIRL iteration = ', i)\n",
    "    \n",
    "            self.Omega_mat = self.omega * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "    \n",
    "            for n in range(101):\n",
    "                if n%100==0:\n",
    "                    print(n)\n",
    "                for t in range(self.num_steps):\n",
    "                    \n",
    "                    # compute rewards obtained at each step for each trajectory\n",
    "                    # given the model parameters\n",
    "                    self.realized_rewards[n,t] = self.compute_reward_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:],\n",
    "                                                                self.data_uvals[n,t,:])\n",
    "                                                                \n",
    "            \n",
    "                    # compute the log-likelihood by looping over trajectories\n",
    "                    self.realized_G_fun[n,t] = self.compute_G_fun_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:],\n",
    "                                                                self.data_uvals[n,t,:])\n",
    "                \n",
    "                \n",
    "                    self.realized_F_fun[n,t] = self.compute_F_fun_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:])\n",
    "                \n",
    "\n",
    "                self.realized_cum_rewards[n] = self.realized_rewards[n,:].sum().clone()\n",
    "                self.realized_G_fun_cum[n] = self.realized_G_fun[n,:].sum().clone()\n",
    "                self.realized_F_fun_cum[n] = self.realized_F_fun[n,:].sum().clone()\n",
    "            \n",
    "            # the negative log-likelihood will not include terms ~ Sigma_p as we do not optimize over its value\n",
    "            loss = - self.beta.clone()*(self.realized_G_fun_cum.sum().clone() - self.realized_F_fun_cum.sum().clone())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            loss.backward() \n",
    "        \n",
    "            optimizer.step()\n",
    "        \n",
    "            print('Iteration = ', i)\n",
    "            print('Loss = ', loss.detach().numpy())\n",
    "        \n",
    "           \n",
    "        print('Done optimizing reward parameters')          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_8XeDlpPQOV"
   },
   "source": [
    "## Simulate portfolio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gy2u0fkyPQOX"
   },
   "source": [
    "### Simulate the market factor under a lognormal distribution with a fixed drift and vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F25fZ-s9PQOb"
   },
   "outputs": [],
   "source": [
    "mu_market = 0.05\n",
    "vol_market = 0.25\n",
    "init_market_val = 100.0\n",
    "\n",
    "r_rf = 0.02  # risk-free rate - the first asset will be cash\n",
    "\n",
    "num_steps = 10 # number of steps for planning horizon\n",
    "dt = 0.25 # quarterly time steps\n",
    "\n",
    "num_risky_assets = 99 # 100\n",
    "\n",
    "returns_market = np.zeros(num_steps)\n",
    "market_vals = np.zeros(num_steps)\n",
    "market_vals[0] = 100.0  # initial value\n",
    "\n",
    "\n",
    "        \n",
    "for t in range(1,num_steps):\n",
    "\n",
    "        rand_norm = np.random.randn()\n",
    "        \n",
    "        # use log-returns of market as 'returns_market'\n",
    "        returns_market[t] = mu_market * dt + vol_market * np.sqrt(dt) * rand_norm\n",
    "        \n",
    "        market_vals[t] = market_vals[t-1] * np.exp((mu_market - 0.5*vol_market**2)*dt + \n",
    "                                                         vol_market*np.sqrt(dt)*rand_norm)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUUBRbwbPQO1"
   },
   "source": [
    "### Simulate market betas and idiosyncratic alphas within pre-defined ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "qraR7fRVPQO4",
    "outputId": "dfb260b8-fa17-4a70-a12d-86a4c2964e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56279548 0.71278128 0.46954189 0.41514778 0.56730598 0.19438644\n",
      " 0.53309895 0.58627183 0.33671025 0.33558516]\n",
      "[ 0.0279932  -0.02035361  0.00318741  0.02451243 -0.00089647  0.1280806\n",
      "  0.07729526  0.10109771 -0.01103058  0.02373868]\n"
     ]
    }
   ],
   "source": [
    "beta_min = 0.05\n",
    "beta_max = 0.85\n",
    "beta_vals = np.random.uniform(low=beta_min, high=beta_max, size=num_risky_assets)\n",
    "\n",
    "alpha_min = - 0.05\n",
    "alpha_max = 0.15\n",
    "alpha_vals = np.random.uniform(low=alpha_min, high=alpha_max, size=num_risky_assets)\n",
    "\n",
    "print(beta_vals[0:10])\n",
    "print(alpha_vals[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DNFzG5dgPQPB"
   },
   "source": [
    "### Simulate time-dependent expected returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a36GRMAlPQPD"
   },
   "outputs": [],
   "source": [
    "# Time-independent expected returns would be equal to alpha + beta * expected_market_return \n",
    "# Make them time-dependent (and correlated with actual returns) as alpha + beta * oracle_market_returns\n",
    "\n",
    "oracle_coeff = 0.2\n",
    "mu_vec = mu_market * np.ones(num_steps)\n",
    "oracle_market_returns = mu_vec * dt + oracle_coeff*(returns_market - mu_vec)\n",
    "\n",
    "expected_risky_returns = np.zeros((num_steps, num_risky_assets))\n",
    "\n",
    "for t in range(num_steps):\n",
    "    expected_risky_returns[t,:] = alpha_vals * dt + beta_vals * oracle_market_returns[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDN_R3znPQPK"
   },
   "source": [
    "### Initial values of all assets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YneTj6IBPQPM"
   },
   "outputs": [],
   "source": [
    "val_min = 20.0\n",
    "val_max = 120.0\n",
    "\n",
    "init_risky_asset_vals = np.random.uniform(low=val_min, high=val_max, size=num_risky_assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqd7WIxbPQPS"
   },
   "source": [
    "### Simulate realized returns and asset prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSL2TU1IPQPW"
   },
   "outputs": [],
   "source": [
    "# Generate realized returns and realized asset values by simulating from a one-factor model \n",
    "# with time-dependent expected returns\n",
    "\n",
    "risky_asset_returns = np.zeros((num_steps, num_risky_assets))\n",
    "risky_asset_vals = np.zeros((num_steps, num_risky_assets))\n",
    "\n",
    "idiosync_vol =  0.05 # vol_market  \n",
    "\n",
    "for t in range(num_steps):\n",
    "    \n",
    "    rand_norm = np.random.randn(num_risky_assets)\n",
    "        \n",
    "    # asset returns are simulated from a one-factor model\n",
    "    risky_asset_returns[t,:] = (expected_risky_returns[t,:] + beta_vals * (returns_market[t] - mu_market * dt) \n",
    "                         + idiosync_vol * np.sqrt(1 - beta_vals**2) * np.sqrt(dt) * rand_norm)\n",
    "        \n",
    "    # asset values\n",
    "    if t == 0:\n",
    "        risky_asset_vals[t,:] = init_risky_asset_vals\n",
    "    else:\n",
    "        risky_asset_vals[t] = risky_asset_vals[t-1] * (1 + risky_asset_returns[t,:])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "YvHvV7B6PQPh",
    "outputId": "3421ed28-a65f-4362-d166-990d22dbf1fd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare the realized and expected returns\n",
    "# Note that they should appear correlated.\n",
    "\n",
    "# pick a random asset ID to show\n",
    "asset_idx =  4 \n",
    "\n",
    "plt.plot(expected_risky_returns[:,asset_idx],label='expected_return')\n",
    "plt.plot(risky_asset_returns[:,asset_idx],label='realized_return',color='r')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Steps')\n",
    "plt.title('Realized returns vs expected returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqAdjcoFPQPp"
   },
   "source": [
    "### Compute the empirical correlation matrix using realized returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sl5JQ3_8PQPs",
    "outputId": "5f93d784-2e83-4a2c-bf2e-23bd42eb5fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 99)\n"
     ]
    }
   ],
   "source": [
    "cov_mat_r = np.cov(risky_asset_returns.T) \n",
    "\n",
    "print(cov_mat_r.shape)\n",
    "\n",
    "D, v = np.linalg.eigh(cov_mat_r)\n",
    "\n",
    "eigenvals = D[::-1]  # put them in a descended order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "H-HzoeHOPQP0",
    "outputId": "96c11580-037d-430d-fe77-5e54635ea519"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.21476127e-01, 8.13620469e-03, 6.64264149e-03, 5.88774245e-03,\n",
       "       5.37519613e-03, 4.87897048e-03, 3.38035639e-03, 3.17834969e-03,\n",
       "       2.54671274e-03, 2.71863304e-17])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eigenvalues: the largest eigenvalue is the market factor \n",
    "eigenvals[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "hFC61Ci94nnd",
    "outputId": "fc1d8cde-9ada-4e39-f6e6-3fe3b55559e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25.1720, -7.1053,  5.5403,  ..., -7.0241, 14.6440, -5.5822],\n",
       "        [-7.1053,  9.6097, -7.0206,  ...,  2.9265, -9.2967,  4.3705],\n",
       "        [ 5.5403, -7.0206, 18.4194,  ...,  2.2626,  4.4261,  5.9377],\n",
       "        ...,\n",
       "        [-7.0241,  2.9265,  2.2626,  ..., 18.1227, -6.0339,  8.4661],\n",
       "        [14.6440, -9.2967,  4.4261,  ..., -6.0339, 23.6777, -5.1727],\n",
       "        [-5.5822,  4.3705,  5.9377,  ...,  8.4661, -5.1727, 10.6778]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_mat_torch = torch.tensor(cov_mat_r)\n",
    "\n",
    "torch.pinverse(cov_mat_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohSC6oDePQP5"
   },
   "source": [
    "### Add a riskless bond as one more asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7nwaM28PQP9"
   },
   "outputs": [],
   "source": [
    "num_assets = num_risky_assets + 1\n",
    "\n",
    "bond_val = 100.0\n",
    "\n",
    "# add the bond to initial assets\n",
    "init_asset_vals = np.hstack((np.array([bond_val]),\n",
    "                            init_risky_asset_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZQgn_CTPQQE"
   },
   "source": [
    "### Make the initial portfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyMLBotBPQQH"
   },
   "outputs": [],
   "source": [
    "# consider here two choices: equal or equally-weighted \n",
    "init_port_choice =  'equal' \n",
    "\n",
    "init_cash = 1000.0\n",
    "init_total_asset = np.sum(init_asset_vals)\n",
    "\n",
    "x_vals_init = np.zeros(num_assets)\n",
    "\n",
    "if init_port_choice == 'equal': \n",
    "    # hold equal amounts of cash in each asset\n",
    "    amount_per_asset = init_cash/num_assets\n",
    "    x_vals_init = amount_per_asset * np.ones(num_assets)\n",
    "\n",
    "elif init_port_choice == 'equally_weighted':\n",
    "    amount_per_asset = init_cash/init_total_asset\n",
    "    x_vals_init = amount_per_asset * init_asset_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zkj8GdPDPQQP"
   },
   "source": [
    "### Make the target portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LyyKCaPGPQQS",
    "outputId": "bda0d8d5-b848-4c2b-c36b-3f2ac1a1bf78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100.0 1541.5835692311712\n"
     ]
    }
   ],
   "source": [
    "# Generate a target portfolio term structure by defining it as \n",
    "# the initial portfolio growing at some fixed and high rate\n",
    "target_portfolio = [init_cash]\n",
    "\n",
    "target_return = 0.15 \n",
    "coeff_target = 1.1 \n",
    "\n",
    "for i in range(1,num_steps):\n",
    "    target_portfolio.append(target_portfolio[i-1]*np.exp(dt * target_return) )\n",
    "    \n",
    "target_portfolio = coeff_target*np.array(target_portfolio)    \n",
    "print(target_portfolio[0], target_portfolio[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2UMA8UwlPQQc"
   },
   "source": [
    "### Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5G_Y87m2PQQe",
    "outputId": "0cf27618-1f16-4676-9741-4e2c3ce8d466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133.1484530668263 3490.3429574618413\n"
     ]
    }
   ],
   "source": [
    "riskfree_rate = 0.02\n",
    "fee_bond = 0.05 # 0.01 # \n",
    "fee_stock = 0.05 # 0.05 # 20.0 # 0.1 # 1.0 # 100 # 1.0 # 0.5 \n",
    "\n",
    "all_fees = np.zeros(num_risky_assets + 1)\n",
    "all_fees[0] = fee_bond\n",
    "all_fees[1:] = fee_stock\n",
    "Omega_mat = np.diag(all_fees)\n",
    "\n",
    "# model parameters\n",
    "lambd = 0.001 \n",
    "Omega_mat = 15.5 * np.diag(all_fees) \n",
    "eta = 1.5 \n",
    "\n",
    "beta = 100.0\n",
    "gamma = 0.95 \n",
    "\n",
    "exp_returns = expected_risky_returns\n",
    "\n",
    "Sigma_r = cov_mat_r\n",
    "\n",
    "# Generate the benchmark target portfolio by growing the initial portfolio value at rate eta\n",
    "target_return =  0.5\n",
    "benchmark_portf = [ init_cash   * np.exp(dt * target_return)]\n",
    "\n",
    "rho = 0.4 \n",
    "\n",
    "for i in range(1,num_steps):\n",
    "    benchmark_portf.append(benchmark_portf[i-1]*np.exp(dt * target_return) )\n",
    "    \n",
    "print(benchmark_portf[0], benchmark_portf[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YT6003xKPQQm"
   },
   "source": [
    "### Simulate portfolio data\n",
    "\n",
    "Produce a list of trajectories, where each trajectory is a list made of state-action pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "So89W92zPLMq"
   },
   "outputs": [],
   "source": [
    "lambd = 0.001 \n",
    "omega = 1.0 \n",
    "beta = 1000.0\n",
    "eta = 1.5 # 1.3 # 1.5 # 1.2\n",
    "rho = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCzgn95o8wUD"
   },
   "outputs": [],
   "source": [
    "reward_params=[lambd, omega, eta, rho]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58NX65KOPQRD"
   },
   "outputs": [],
   "source": [
    "# Create a G-learner\n",
    "G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                 reward_params,  \n",
    "                 beta,                \n",
    "                 benchmark_portf,\n",
    "                 gamma, \n",
    "                 num_risky_assets,\n",
    "                 riskfree_rate,\n",
    "                 expected_risky_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns of risky matrix                    \n",
    "                 x_vals_init, # array of initial values of len (num_stocks+1)\n",
    "                 use_for_WM = True) # use for wealth management tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mhy6IPKs7nRq",
    "outputId": "78fbc7d6-2869-48a9-f754-c9ed7c5127c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n"
     ]
    }
   ],
   "source": [
    "G_learner.reset_prior_policy()\n",
    "error_tol=1.e-8 \n",
    "max_iter_RL = 200\n",
    "G_learner.G_learning(error_tol, max_iter_RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZJbI_k9mPQQo",
    "outputId": "45fc8ba1-fb6d-450b-8502-b96ffa56419a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done simulating trajectories in 15.149038 sec\n"
     ]
    }
   ],
   "source": [
    "num_sim = 1000\n",
    "trajs = []\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "t_0 = time.time()\n",
    "\n",
    "\n",
    "x_vals = [x_vals_init]\n",
    "returns_all = []\n",
    "for n in range(num_sim):\n",
    "    this_traj = []\n",
    "    x_t = x_vals_init[:]\n",
    "    returns_array = []\n",
    "    for t in range(0,num_steps):\n",
    "        mu_t = G_learner.u_bar_prior[t,:] + G_learner.v_bar_prior[t,:].mv(torch.tensor(x_t))\n",
    "        u_t = np.random.multivariate_normal(mu_t.detach().numpy(), G_learner.Sigma_prior[t,:].detach().numpy())\n",
    "        # compute new values of x_t\n",
    "\n",
    "        x_next = x_t +u_t\n",
    "        # grow this with random return\n",
    "        \n",
    "        idiosync_vol =  0.05 # vol_market     \n",
    "        rand_norm = np.random.randn(num_risky_assets)\n",
    "        \n",
    "        # asset returns are simulated from a one-factor model\n",
    "        risky_asset_returns = (expected_risky_returns[t,:] + beta_vals * (returns_market[t] - mu_market * dt) \n",
    "                         + idiosync_vol * np.sqrt(1 - beta_vals**2) * np.sqrt(dt) * rand_norm)\n",
    "        \n",
    "        returns = np.hstack((riskfree_rate*dt, risky_asset_returns))\n",
    "        \n",
    "        x_next = (1+returns)*x_next\n",
    "        port_returns=(x_next.sum() -x_t.sum() -np.sum(u_t) - 0.015*np.abs(u_t).sum())/x_t.sum()\n",
    "        \n",
    "        this_traj.append((x_t, u_t))\n",
    "        \n",
    "        # rename\n",
    "        x_t = x_next\n",
    "        returns_array.append(port_returns) \n",
    "    # end the loop over time steps\n",
    "    trajs.append(this_traj)\n",
    "    returns_all.append(returns_array)\n",
    "\n",
    "print('Done simulating trajectories in %f sec'% (time.time() - t_0))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpnem5H5vZ5H"
   },
   "source": [
    "### Calculate performance of G-learner (Diagnostics only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Z731JGirsdV"
   },
   "outputs": [],
   "source": [
    "returns_all_G = returns_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UIYUAIWvO04X",
    "outputId": "7e9f5055-b670-4a22-c6cf-d8082c03c9c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2571853631721223\n"
     ]
    }
   ],
   "source": [
    "SR_G = 0\n",
    "for i in range(num_sim):\n",
    "    SR_G += (np.mean(returns_all_G[i])-riskfree_rate*dt)/np.std(returns_all_G[i])\n",
    "\n",
    "SR_G/=num_sim\n",
    "print(SR_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TT9dIJP_t0K_"
   },
   "outputs": [],
   "source": [
    "r_G = np.array([0]*num_steps, dtype='float64')\n",
    "for n in range(num_steps):\n",
    "    for i in range(num_sim):\n",
    "        r_G[n]+=returns_all_G[i][n]\n",
    "    r_G[n]/=num_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "KAdjSXoHui7R",
    "outputId": "9f5ea1f7-38b7-4aff-ef5a-e9ef9788651d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(r_G, label='G-learning (' + str(np.round(SR_G,3)) + ')', color='red')\n",
    "\n",
    "plt.xlabel('time (quarters)')\n",
    "plt.ylabel('Sample Mean Returns');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTWkeFsDPQQu"
   },
   "outputs": [],
   "source": [
    "np.save('State_act_trajs.npy', trajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "SNtCvuFMPQQ0",
    "outputId": "2f12bdef-6454-4811-9d4b-5ee128b9f596"
   },
   "outputs": [],
   "source": [
    "#trajs = np.load('State_act_trajs.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RysHrlg3yDyP"
   },
   "source": [
    "## Plot rewards and cash installments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the learng rewards and observe the required cash installements from the G-learner model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcU6SnH5JsC0"
   },
   "outputs": [],
   "source": [
    "reward_params=[lambd, omega, eta, rho]\n",
    "G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                                      reward_params,\n",
    "                                      beta,\n",
    "                                      benchmark_portf,\n",
    "                                      gamma,\n",
    "                                      num_risky_assets,\n",
    "                                      riskfree_rate,\n",
    "                                      expected_risky_returns,\n",
    "                                      Sigma_r,\n",
    "                                      x_vals_init,\n",
    "                                      use_for_WM=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "JiAyHd94PQRI",
    "outputId": "5c91e69d-0356-46c0-9013-71056b894952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n"
     ]
    }
   ],
   "source": [
    "err_tol = 1.e-10 \n",
    "max_iter = 500\n",
    "\n",
    "t_0 = time.time()\n",
    "G_learner.reset_prior_policy()\n",
    "G_learner.G_learning(err_tol=err_tol, max_iter=max_iter)\n",
    "num_trajs = len(trajs)\n",
    "\n",
    "data_xvals = torch.zeros(num_trajs,  num_steps, num_assets, dtype=torch.float64, requires_grad=False)\n",
    "data_uvals = torch.zeros(num_trajs,  num_steps, num_assets, dtype=torch.float64, requires_grad=False)\n",
    "num_trajs = len(trajs)       \n",
    "for n in range(num_trajs):\n",
    "        for t in range(num_steps):\n",
    "            data_xvals[n,t,:] = torch.tensor(trajs[n][t][0],dtype=torch.float64)\n",
    "            data_uvals[n,t,:] = torch.tensor(trajs[n][t][1],dtype=torch.float64)\n",
    "\n",
    "realized_rewards = torch.zeros(num_trajs, num_steps, dtype=torch.float64, requires_grad=False)\n",
    "realized_cum_rewards = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=False)\n",
    "\n",
    "realized_G_fun = torch.zeros(num_trajs, num_steps, dtype=torch.float64, requires_grad=False)\n",
    "realized_F_fun  = torch.zeros(num_trajs,  num_steps, dtype=torch.float64, requires_grad=False)\n",
    "\n",
    "realized_G_fun_cum = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=False)  \n",
    "realized_F_fun_cum = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=False)         \n",
    "\n",
    "# compute the rewards and realized values of G- and F-functions from \n",
    "# all trajectories\n",
    "for n in range(num_trajs):\n",
    "    for t in range(num_steps):\n",
    "                \n",
    "                realized_rewards[n,t] = G_learner.compute_reward_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "                realized_G_fun[n,t] = G_learner.compute_G_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "                realized_F_fun[n,t] = G_learner.compute_F_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:])\n",
    "                 \n",
    "    realized_cum_rewards[n] = realized_rewards[n,:].sum()\n",
    "    realized_G_fun_cum[n] = realized_G_fun[n,:].sum()\n",
    "    realized_F_fun_cum[n] = realized_F_fun[n,:].sum()\n",
    "\n",
    "print('Done in %f sec'% (time.time() - t_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "txOKhgy7PQRm",
    "outputId": "6bc8c687-3ac3-4aaf-b06a-c5ef4d89df77"
   },
   "outputs": [],
   "source": [
    "sim_idx =  999\n",
    "\n",
    "realized_rewards = realized_rewards[sim_idx,:].detach().numpy()\n",
    "plt.plot(realized_rewards,label='realized rewards')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Steps')\n",
    "plt.title('Realized rewards');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "jUeIUzmMPQRU",
    "outputId": "8afb4655-b13e-410a-89d6-bcfe26675163"
   },
   "outputs": [],
   "source": [
    "G_learner.project_cash_injections()\n",
    "\n",
    "eta_ = G_learner.eta.detach().numpy()\n",
    "realized_target_portf = eta_ * G_learner.expected_portf_val.numpy()\n",
    "\n",
    "plt.plot(G_learner.expected_c_t, label='optimal cash installments')\n",
    "plt.plot(G_learner.expected_portf_val, label='expected portfolio value')\n",
    "plt.plot(realized_target_portf,label='realized target portfolio',color='r')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time Steps')\n",
    "plt.title('Optimal cash installment and portfolio value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the histogram of cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "id": "LDVVOMCLNvM1",
    "outputId": "4d64768d-98f0-4c1b-d597-179aac815f6d"
   },
   "outputs": [],
   "source": [
    "plt.hist(realized_cum_rewards.detach().numpy(), bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Wealth_Management_GIRL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
