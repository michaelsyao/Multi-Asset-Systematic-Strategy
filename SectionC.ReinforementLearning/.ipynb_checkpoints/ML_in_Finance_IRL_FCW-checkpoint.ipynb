{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Reinforcement Learning for Financial Cliff Walking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains implementations of three IRL algorithms for the Financial Cliff Walking (FCW) problem:\n",
    "\n",
    "1. Max Causal Entropy IRL\n",
    "\n",
    "2. IRL from Failure (IRLF) \n",
    "\n",
    "3. T-REX\n",
    "\n",
    "The notebook is self-contained, training data and ground truth are generated within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# To ignore warnings that are annoying.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "# N - World height\n",
    "# T - World width\n",
    "WORLD_HEIGHT = 4\n",
    "WORLD_WIDTH = 12\n",
    "\n",
    "# Probability for exploration - epsilon\n",
    "EPSILON = 0.1\n",
    "# Step size\n",
    "ALPHA = 0.001\n",
    "# Gamma - discount factor - for Q-Learning, Sarsa and Expected Sarsa\n",
    "GAMMA = 0.9\n",
    "\n",
    "# Actions - ACTION_UP is a+ (adding a deposit), ACTION_DOWN is a-(redeeming a deposit) and \n",
    "# ACTION_ZERO is a0 (leaving the account as it is).\n",
    "ACTION_UP = 0\n",
    "ACTION_DOWN = 1\n",
    "ACTION_ZERO = 2\n",
    "ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_ZERO]\n",
    "\n",
    "# Initial and Goal states\n",
    "START = [1,0]\n",
    "GOAL = [0, WORLD_WIDTH-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions determining the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step function that describes how the next state is obtained from the current state and the action \n",
    "# taken. The function returns the next state and the reward obtained.\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "\n",
    "    if state[0] == 0 and (state[1] > 0): #  and state[1] < WORLD_WIDTH - 2):\n",
    "        # remain in the bankruptcy state\n",
    "        next_state =  [0, min(j + 1, WORLD_WIDTH - 1)]\n",
    "        reward = 0 \n",
    "        return next_state, reward\n",
    "    \n",
    "    # if at the final time, next state is the same, and reward is zero\n",
    "    if state[1] == WORLD_WIDTH - 1:\n",
    "        next_state = [i,state[1]]\n",
    "        reward = 0\n",
    "        return next_state, reward\n",
    "    \n",
    "    if action == ACTION_UP:\n",
    "        next_state = [min(i + 1, WORLD_HEIGHT-1), min(j + 1, WORLD_WIDTH - 1)]\n",
    "    elif action == ACTION_DOWN:\n",
    "        next_state = [max(i - 1, 0), min(j + 1, WORLD_WIDTH - 1)]\n",
    "    elif action == ACTION_ZERO:\n",
    "        next_state = [i, min(j + 1, WORLD_WIDTH - 1)]\n",
    "    else:\n",
    "        assert False\n",
    "    \n",
    "    # The reward is -1 for actions ACTION_UP and ACTION_DOWN. This is done to keep transactions to a minimum.\n",
    "    reward = -1\n",
    "    \n",
    "    # ACTION_ZERO gets a zero reward since we want to minimize the number of transactions\n",
    "    if action == ACTION_ZERO:\n",
    "        reward = 0\n",
    "    \n",
    "    # Exceptions are \n",
    "    # i) If bankruptcy happens before WORLD_WIDTH time steps\n",
    "    # ii) No deposit at initial state\n",
    "    # iii) Redemption at initial state!\n",
    "    # iv) Any action carried out from a bankrupt state\n",
    "    if ((action == ACTION_DOWN and i == 1 and 1 <= j < 10) or (\n",
    "        action == ACTION_ZERO and state == START) or (\n",
    "        action == ACTION_DOWN and state == START )) or (\n",
    "        i == 0 and 1 <= j <= 10):    \n",
    "            reward = -100\n",
    "        \n",
    "    # Next exception is when we get to the final time step.\n",
    "    if state[0] != 0 and (next_state[1] == WORLD_WIDTH - 1): \n",
    "        # override a random action by a deterministic action=ACTION_DOWN\n",
    "        if (next_state[0] == 0): # Action resulted in ending with zero balance in final time step\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = -10   \n",
    "        \n",
    "    return next_state, reward\n",
    "\n",
    "# Choose an action based on epsilon greedy algorithm\n",
    "def choose_action(state, q_value, eps=EPSILON):\n",
    "    if np.random.binomial(1, eps) == 1:\n",
    "        action = np.random.choice(ACTIONS)\n",
    "    else:\n",
    "        values_ = q_value[state[0], state[1], :]\n",
    "        action = np.random.choice([action_ for action_, value_ in enumerate(values_) \n",
    "                                 if value_ == np.max(values_)])\n",
    "    # From bankrupt state there is no meaningful action, so we will assign 'Z' by convention.\n",
    "    if state[0] == 0 and state[1] > 0:\n",
    "        action = ACTION_ZERO\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for MaxEnt IRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos2idx(pos):\n",
    "    \"\"\"\n",
    "    input:\n",
    "      column-major 2d position\n",
    "    returns:\n",
    "      1d index\n",
    "    \"\"\"\n",
    "    return np.ravel_multi_index((pos[0], pos[1]), (WORLD_HEIGHT, WORLD_WIDTH))\n",
    "\n",
    "def idx2pos(idx):\n",
    "        \n",
    "    \"\"\"\n",
    "    input:\n",
    "      1d idx\n",
    "    returns:\n",
    "      2d column-major position\n",
    "    \"\"\"\n",
    "    \n",
    "    pos = [np.unravel_index(idx, (WORLD_HEIGHT, WORLD_WIDTH))[0],\n",
    "           np.unravel_index(idx, (WORLD_HEIGHT, WORLD_WIDTH))[1]]\n",
    "    return pos \n",
    "\n",
    "'''\n",
    "Functions get_transition_mat, value_iteration, compute_state_visition_freq\n",
    "defined below are adapted from this reference:\n",
    "\n",
    "https://github.com/yrlu/irl-imitation\n",
    "\n",
    "Implementation of maximum entropy inverse reinforcement learning in\n",
    "  Ziebart et al. 2008 paper: Maximum Entropy Inverse Reinforcement Learning\n",
    "  https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf\n",
    "\n",
    "By Yiren Lu (luyirenmax@gmail.com), May 2017\n",
    "'''\n",
    "\n",
    "def get_transition_mat():\n",
    "    \n",
    "    \"\"\"\n",
    "    get transition dynamics of the gridworld\n",
    "\n",
    "    return:\n",
    "      P_a         NxNxN_ACTIONS transition probabilities matrix - \n",
    "                    P_a[s0, s1, a] is the transition prob of \n",
    "                    landing at state s1 when taking action \n",
    "                    a at state s0\n",
    "                    \n",
    "    Probabilities are ones for transitions to states produced by function step(state,action)\n",
    "    \"\"\"\n",
    "    n_states = WORLD_HEIGHT * WORLD_WIDTH\n",
    "    n_actions = len(ACTIONS)\n",
    "    \n",
    "    P_a = np.zeros((n_states, n_states, n_actions))\n",
    "    \n",
    "    for si in range(1, n_states): \n",
    "        for a in ACTIONS:            \n",
    "            next_state, reward = step(idx2pos(si),a)\n",
    "            sj = pos2idx(next_state)                         \n",
    "            P_a[int(si), int(sj), int(a)] = 1\n",
    "\n",
    "    return P_a\n",
    "\n",
    "def get_true_rewards():\n",
    "    \"\"\"\n",
    "    get the reward matrix of shape n_states x n_actions\n",
    "    \"\"\"\n",
    "    n_states = WORLD_HEIGHT * WORLD_WIDTH\n",
    "    n_actions = len(ACTIONS)\n",
    "    \n",
    "    gt_rewards = np.zeros((n_states, n_actions))\n",
    "    for si in range(n_states):\n",
    "        for a in ACTIONS:            \n",
    "            next_state, reward = step(idx2pos(si),a)                        \n",
    "            gt_rewards[int(si), int(a)] = reward\n",
    "    \n",
    "    return gt_rewards\n",
    "    \n",
    "def get_rewards_on_rectangle(rewards, action): # matrix of size n_state x n_action\n",
    "                            \n",
    "    # convert rewards into rectangular form WORLD_HEIGHT x WORLD_WIDTH\n",
    "    rewards_rectangle = np.zeros((WORLD_HEIGHT, WORLD_WIDTH))\n",
    "    n_states = rewards.shape[0]\n",
    "    \n",
    "    for idx in range(n_states):\n",
    "        height, width = idx2pos(idx)\n",
    "        rewards_rectangle[height,width] = rewards[idx,action]\n",
    "    \n",
    "    return rewards_rectangle\n",
    "\n",
    "def get_values_on_rectangle(values): # here values are given by the vector of size n_states\n",
    "    \n",
    "    values_rectangle = np.zeros((WORLD_HEIGHT, WORLD_WIDTH))\n",
    "    n_states = len(values)\n",
    "    \n",
    "    for idx in range(n_states):\n",
    "        height, width = idx2pos(idx)\n",
    "        values_rectangle[height,width] = values[idx]\n",
    "    \n",
    "    return values_rectangle\n",
    "    \n",
    "def value_iteration(P_a, rewards, gamma, error=0.01, deterministic=False):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    static value iteration function\n",
    "  \n",
    "    inputs:\n",
    "    P_a         N x N x N_ACTIONS transition probabilities matrix - \n",
    "                              P_a[s0, s1, a] is the transition prob of \n",
    "                              landing at state s1 when taking action \n",
    "                              a at state s0\n",
    "    rewards     N x N_ACTIONS matrix - rewards for all the state-action combinations\n",
    "    gamma       float - RL discount\n",
    "    error       float - threshold for a stop\n",
    "    deterministic   bool - to return deterministic policy or stochastic policy\n",
    "  \n",
    "    returns:\n",
    "    values    Nx1 matrix - estimated values\n",
    "    policy    Nx1 (NxN_ACTIONS if non-det) matrix - policy\n",
    "    \"\"\"\n",
    "    n_states, _, n_actions = np.shape(P_a)\n",
    "\n",
    "    values = np.zeros([n_states])\n",
    "\n",
    "    # estimate values\n",
    "    while True:\n",
    "        values_tmp = values.copy()\n",
    "\n",
    "        for s in range(n_states):\n",
    "            v_s = []\n",
    "            values[s] = max([sum([P_a[s, s1, a]*(rewards[s,a] + gamma*values_tmp[s1]) for s1 in range(n_states)]) \n",
    "                             for a in ACTIONS])    \n",
    "        \n",
    "        if max([abs(values[s] - values_tmp[s]) for s in range(n_states)]) < error:\n",
    "            break\n",
    "\n",
    "\n",
    "    if deterministic:\n",
    "        # generate deterministic policy\n",
    "        policy = np.zeros([n_states])\n",
    "        for s in range(n_states):\n",
    "            policy[s] = np.argmax([sum([P_a[s, s1, a]*(rewards[s,a]+gamma*values[s1]) \n",
    "                                  for s1 in range(n_states)]) \n",
    "                                  for a in ACTIONS])\n",
    "\n",
    "        return values, policy\n",
    "    else:\n",
    "        # generate stochastic policy\n",
    "        policy = np.zeros([n_states, n_actions])\n",
    "        for s in range(n_states):\n",
    "            v_s = np.array([sum([P_a[s, s1, a]*(rewards[s,a] + gamma*values[s1]) for s1 in range(n_states)]) \n",
    "                            for a in ACTIONS])\n",
    "            policy[s,:] = np.transpose(v_s/np.sum(v_s))\n",
    "        return values, policy\n",
    "    \n",
    "    \n",
    "def soft_Q_iteration(rewards, gamma, beta, learn_rate_Q, n_states, n_actions, \n",
    "                     trajs, error=0.01, max_iter = 500):\n",
    "    \"\"\"\n",
    "    Performs soft Q-iteration based on observed samples \n",
    "    Returns: the soft-Q function, soft-V function, and soft-Q policy\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    T = trajs.shape[1] \n",
    "    n_traj = trajs.shape[0]\n",
    "    \n",
    "    discounts = np.ones(T)\n",
    "    for t in range(1,T):\n",
    "        discounts[t] *= gamma\n",
    "        \n",
    "    # initialize\n",
    "    soft_Q_vals =  np.zeros((n_states, n_actions))\n",
    "    soft_Q_policy = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    iter_idx = 0\n",
    "    \n",
    "    # soft-Q iteration\n",
    "    while True:\n",
    "        soft_Q_vals_tmp = soft_Q_vals.copy()\n",
    "        \n",
    "        for episode in range(n_traj):\n",
    "            for t in range(T):\n",
    "                state = trajs[episode,t,0]\n",
    "                action = trajs[episode,t,1]\n",
    "                next_state = trajs[episode,t,2]\n",
    "                reward = rewards[state, action]\n",
    "                \n",
    "                soft_V_vals_next = (1.0/beta) * np.log(np.sum(np.exp(beta*soft_Q_vals[next_state,:])))\n",
    "                soft_Q_vals[state, action] += learn_rate_Q*(reward + gamma * soft_V_vals_next \n",
    "                                                            - soft_Q_vals[state, action])\n",
    "        \n",
    "        iter_idx += 1\n",
    "        if np.max(soft_Q_vals - soft_Q_vals_tmp) < error or iter_idx >= max_iter:\n",
    "            break\n",
    "\n",
    "    # soft V-function        \n",
    "    soft_V_vals = (1.0/beta) * np.log(np.sum(np.exp(beta*soft_Q_vals),axis=1))\n",
    "    \n",
    "    # stochastic policy\n",
    "    policy = np.exp(beta*(soft_Q_vals - np.tile(soft_V_vals[:, np.newaxis],(1,n_actions) )))\n",
    "            \n",
    "    return soft_Q_vals, soft_V_vals, policy\n",
    "\n",
    "\n",
    "def soft_Q_iteration_mb(P_a, rewards, gamma, beta, n_states, n_actions, error=0.01):\n",
    "    \"\"\"\n",
    "    Soft Q-iteration based on a transition model (model-based soft Q-iteration)\n",
    "    \"\"\"\n",
    "    \n",
    "    Q_values = np.zeros((n_states, n_actions))\n",
    "\n",
    "    # estimate Q-values\n",
    "    while True:\n",
    "        Q_values_tmp = Q_values.copy()\n",
    "\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):   \n",
    "                Q_values[s,a] = np.sum(P_a[s,:,a]*(rewards[s,a]\n",
    "                                        + (gamma/beta)*np.log(np.sum(np.exp(beta*Q_values_tmp[:,:]),axis=1))))\n",
    "        if np.max(np.abs(Q_values - Q_values_tmp)) < error:\n",
    "            break\n",
    "\n",
    "    # soft V-function        \n",
    "    soft_V_vals = (1.0/beta) * np.log(np.sum(np.exp(beta*Q_values),axis=1))\n",
    "    \n",
    "    # stochastic policy\n",
    "    policy = np.exp(beta*(Q_values - np.tile(soft_V_vals[:, np.newaxis],(1,n_actions) )))\n",
    "            \n",
    "    return Q_values, soft_V_vals, policy            \n",
    "        \n",
    "def compute_state_visition_freq(P_a, gamma, T, policy, deterministic=False):\n",
    "    \n",
    "    \"\"\"compute the expected states visition frequency p(s| theta, T) \n",
    "    using dynamic programming\n",
    "\n",
    "    inputs:\n",
    "    P_a     N x N x N_ACTIONS matrix - transition dynamics\n",
    "    gamma   float - discount factor\n",
    "    trajs   list of list of Steps - collected from expert\n",
    "    policy  N x 1 vector (or NxN_ACTIONS if deterministic=False) - policy\n",
    "    T       number of time steps\n",
    "  \n",
    "    returns:\n",
    "    svf       N x 1 vector - state visitation frequencies\n",
    "    savf      N x N_ACTION - state-action visitation frequencies\n",
    "    \"\"\"\n",
    "    n_states, _, n_actions = np.shape(P_a)\n",
    "    \n",
    "    # mu[s, t] is the prob of visiting state s at time t\n",
    "    mu = np.zeros([n_states, T]) \n",
    "    \n",
    "    # as all trajectories in our case start at [0,0], initialize the time-dependent state density at t = 0\n",
    "    mu[pos2idx(START),0] = 1\n",
    "\n",
    "    # compute mu[s,t] for other times \n",
    "    for s in range(n_states):\n",
    "        for t in range(T-1):\n",
    "            if deterministic:\n",
    "                mu[s, t+1] = sum([mu[pre_s, t]*P_a[pre_s, s, int(policy[pre_s])] \n",
    "                                  for pre_s in range(n_states)])\n",
    "            else:\n",
    "                mu[s, t+1] = sum([sum([mu[pre_s, t]*P_a[pre_s, s, a1]*policy[pre_s, a1] \n",
    "                                       for a1 in ACTIONS]) for pre_s in range(n_states)])\n",
    "\n",
    "    \n",
    "    # array of discount factors\n",
    "    discounts = np.ones(T)\n",
    "    for i in range(1,T):\n",
    "        discounts[i] = gamma * discounts[i-1]\n",
    "    \n",
    "    # state visitation frequencies\n",
    "    svf = np.sum(mu * discounts, 1)\n",
    "        \n",
    "    # state-action visitation frequencies\n",
    "    savf = np.tile(svf[:,np.newaxis],(1,n_actions)) * policy\n",
    "    \n",
    "    return svf, savf\n",
    "\n",
    "\n",
    "def maxent_irl(feat_map, P_a, gamma, beta, lr, lr_Q, trajs, n_iters):\n",
    "    \n",
    "    \"\"\"\n",
    "    Maximum Entropy Inverse Reinforcement Learning (Maxent IRL)\n",
    "\n",
    "    inputs:\n",
    "        feat_map    NxD matrix - the features for each state\n",
    "        P_a         NxNxN_ACTIONS matrix - P_a[s0, s1, a] is the transition prob of \n",
    "                                           landing at state s1 when taking action \n",
    "                                           a at state s0\n",
    "        gamma       float - RL discount factor\n",
    "        beta        regularization parameter for soft Q-learning\n",
    "        lr          learning rate\n",
    "        lr_Q        learning rate for soft Q-learning\n",
    "        trajs       a list of demonstrations\n",
    "        \n",
    "        n_iters     int - number of optimization steps\n",
    "\n",
    "    returns\n",
    "        rewards     Nx1 vector - recoverred state rewards\n",
    "    \"\"\"\n",
    "    n_states, _, n_actions = np.shape(P_a)\n",
    "\n",
    "    # init parameters\n",
    "    \n",
    "    # theta is a matrix of parameters of shape n_states x n_actions, for actions = 0,1,2\n",
    "    theta = np.random.uniform(low=-0.1,high=0.1, size=(feature_matrix.shape[1],n_actions))    \n",
    "\n",
    "    # calc feature expectations\n",
    "    feat_exp = np.zeros((feat_map.shape[1],n_actions))\n",
    "    \n",
    "    T = trajs.shape[1]\n",
    "    n_traj = trajs.shape[0]\n",
    "    \n",
    "    discounts = np.ones(T)\n",
    "    for t in range(1,T):\n",
    "        discounts[t] *= gamma\n",
    "    \n",
    "    for episode in range(n_traj):\n",
    "        for t in range(T):\n",
    "            for a in ACTIONS:\n",
    "                state = trajs[episode,t,0]\n",
    "                action = trajs[episode,t,1]\n",
    "                feat_exp[state, action] += discounts[t]\n",
    "    \n",
    "    feat_exp = feat_exp/n_traj \n",
    "    \n",
    "    # training\n",
    "    for iteration in range(n_iters):\n",
    "  \n",
    "        if iteration % (n_iters/20) == 0:\n",
    "            print('iteration: {}/{}'.format(iteration, n_iters))\n",
    "    \n",
    "        # compute reward function as a matrix of shape n_states x n_actions, for actions = 0,1,2\n",
    "        rewards = np.dot(feat_map, theta)\n",
    "        \n",
    "        # try computing policy using model-based soft Q-iteration\n",
    "        _, _, policy = soft_Q_iteration_mb(P_a, rewards, gamma, beta, \n",
    "                                           n_states, n_actions, error=0.01)\n",
    "    \n",
    "        # compute state-action visition frequences\n",
    "        _, savf = compute_state_visition_freq(P_a, gamma, T, policy, deterministic=False)\n",
    "            \n",
    "        # compute gradients\n",
    "        grad = feat_exp - feat_map.T.dot(savf) # now gradient has shape n_states x n_actions\n",
    "\n",
    "        # update params\n",
    "        theta += lr * grad\n",
    "\n",
    "    rewards = np.dot(feat_map, theta)\n",
    "    return rewards \n",
    "\n",
    "# use this function from irl-imitation for displaying results\n",
    "def heatmap2d(hm_mat, title='', block=True, fig_num=1, text=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Display heatmap\n",
    "    input:\n",
    "        hm_mat:   mxn 2d np array\n",
    "    \"\"\"\n",
    "    print ('map shape: {}, data type: {}'.format(hm_mat.shape, hm_mat.dtype))\n",
    "\n",
    "    if block:\n",
    "        plt.figure(fig_num)\n",
    "        plt.clf()\n",
    "  \n",
    "    plt.imshow(hm_mat, interpolation='nearest',origin=\"lower\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "  \n",
    "    if text:\n",
    "        for y in range(hm_mat.shape[0]):\n",
    "            for x in range(hm_mat.shape[1]):\n",
    "                plt.text(x, y, '%.1f' % hm_mat[y, x],\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center',\n",
    "                     )\n",
    "\n",
    "    if block:\n",
    "        plt.ion()\n",
    "        print ('press enter to continue')\n",
    "        plt.show()\n",
    "        raw_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRL from Failure (IRLF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def irl_from_failure(feat_map, P_a, gamma, \n",
    "                     beta,\n",
    "                     trajs_success, \n",
    "                     trajs_failure,\n",
    "                     lr,\n",
    "                     lr_Q,\n",
    "                     alpha_lam,\n",
    "                     lam,\n",
    "                     lam_min, \n",
    "                     n_iters):\n",
    "    \n",
    "    \"\"\"\n",
    "    Maximum Entropy Inverse Reinforcement Learning (Maxent IRL)\n",
    "\n",
    "    inputs:\n",
    "        feat_map    NxD matrix - the features for each state\n",
    "        P_a         NxNxN_ACTIONS matrix - P_a[s0, s1, a] is the transition prob of \n",
    "                                           landing at state s1 when taking action \n",
    "                                           a at state s0\n",
    "        gamma               float - RL discount factor\n",
    "        trajs_success       successful demonstrations\n",
    "        trajs_failure       failed demonstrations\n",
    "        lr          float - learning rate\n",
    "        n_iters     int - number of optimization steps\n",
    "\n",
    "    returns\n",
    "        rewards     Nx1 vector - recoverred state rewards\n",
    "    \"\"\"\n",
    "    n_states, _, n_actions = np.shape(P_a)\n",
    "\n",
    "    # init parameters\n",
    "    \n",
    "    # theta_s and theta_f are sets of parameters for successful and failed trajectories\n",
    "    \n",
    "    # theta_s is a matrix of parameters of shape n_states x n_actions, for actions = 0,1,2\n",
    "    theta_s = np.random.uniform(low=-0.1,high=0.1, size=(feature_matrix.shape[1],n_actions))    \n",
    "    theta_f = np.zeros((feature_matrix.shape[1], n_actions))\n",
    "    \n",
    "    # calc feature expectations\n",
    "    feat_exp_s = np.zeros((feat_map.shape[1],n_actions))\n",
    "    feat_exp_f = np.zeros((feat_map.shape[1],n_actions))\n",
    "    \n",
    "    T_s = trajs_success.shape[1]\n",
    "    n_traj_s = trajs_success.shape[0]\n",
    "    T_f = trajs_failure.shape[1] \n",
    "    n_traj_f = trajs_failure.shape[0]\n",
    "    \n",
    "    # discount factors\n",
    "    T = T_s\n",
    "    discounts = np.ones(T)\n",
    "    for t in range(1,T):\n",
    "        discounts[t] *= gamma\n",
    "    \n",
    "    # empirical feature expectations for successful trajectories\n",
    "    for episode in range(n_traj_s):\n",
    "        for t in range(T):\n",
    "            for a in ACTIONS:\n",
    "                state = trajs_success[episode,t,0]\n",
    "                action = trajs_success[episode,t,1]\n",
    "                feat_exp_s[state, action] += discounts[t]\n",
    "    \n",
    "    feat_exp_s = feat_exp_s/n_traj_s \n",
    "    \n",
    "    # empirical feature expectations for failed trajectories\n",
    "    for episode in range(n_traj_f):\n",
    "        for t in range(T):\n",
    "            for a in ACTIONS:\n",
    "                state = trajs_failure[episode,t,0]\n",
    "                action = trajs_failure[episode,t,1]\n",
    "                feat_exp_f[state, action] += discounts[t]\n",
    "    \n",
    "    feat_exp_f = feat_exp_f/n_traj_f \n",
    "    \n",
    "    # training\n",
    "    for iteration in range(n_iters):\n",
    "  \n",
    "        if iteration % (n_iters/20) == 0:\n",
    "            print('iteration: {}/{}'.format(iteration, n_iters))\n",
    "    \n",
    "        # compute reward function as a matrix of shape n_states x n_actions, for actions = 0,1,2\n",
    "        rewards = np.dot(feat_map, theta_s + theta_f)\n",
    "\n",
    "        # compute policy (using only successful trajectories?)\n",
    "        \n",
    "        # try computing policy using model-based soft Q-iteration\n",
    "        _, _, policy = soft_Q_iteration_mb(P_a, rewards, gamma, beta, \n",
    "                                           n_states, n_actions, error=0.01)\n",
    "    \n",
    "        # compute state-action visition frequency\n",
    "        _, savf_s = compute_state_visition_freq(P_a, gamma, T_s, policy, deterministic=False)\n",
    "            \n",
    "        # as both successful and failed trajectories start with the same initial state and \n",
    "        # have the same length, the state-action visitation frequency for failed trajectories is the same\n",
    "        savf_f = savf_s.copy()\n",
    "            \n",
    "        # compute gradients\n",
    "        grad_theta_s = feat_exp_s - feat_map.T.dot(savf_s) # now gradient has shape n_states x n_actions\n",
    "\n",
    "        # update params\n",
    "        theta_s += lr * grad_theta_s\n",
    "        \n",
    "        # recompute weights theta_f\n",
    "        theta_f = (savf_f - feat_exp_f)/lam\n",
    "        \n",
    "        if lam > lam_min:\n",
    "            lam = alpha_lam * lam\n",
    "\n",
    "    return rewards, policy, theta_s, theta_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create trajectories made of tuples $ (s_t, a_t, s_{t+1}, r_t) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_np_trajectories 10000\n",
      "np_trajectories.shape =  (10000, 11, 4)\n",
      "Done in 7.03 sec\n"
     ]
    }
   ],
   "source": [
    "num_traj = 10000\n",
    "num_steps = WORLD_WIDTH\n",
    "\n",
    "trajectories = []\n",
    "rewards_on_traj = []\n",
    "\n",
    "episode_step = 0\n",
    "eps = 0.2  # parameter for randomization of policy\n",
    "\n",
    "t_0 = time.time()\n",
    "\n",
    "for episode in range(num_traj): \n",
    "    trajectory = []\n",
    "    rewards = 0\n",
    "    \n",
    "    for i in range(num_steps-1):\n",
    "        if i == 0:\n",
    "            # initially make the move up\n",
    "            state = START\n",
    "            # if the initial state is [0,1] instead of [0,0], we are not forced to only take action_up\n",
    "            action = np.random.choice(ACTIONS, p=[eps,0,1 - eps]) \n",
    "            new_state, reward = step(START, ACTION_UP)\n",
    "        elif (0 < i and i < num_steps-2):\n",
    "            if state[0] > 0:\n",
    "                action = np.random.choice(ACTIONS, p=[eps/2, eps/2, 1 - eps])    \n",
    "                new_state, reward = step(state, action)\n",
    "            else: # already in the bankruptcy state\n",
    "                action = ACTION_ZERO\n",
    "                new_state = [0, min(state[1] + 1, WORLD_WIDTH - 1)]\n",
    "                reward = 0\n",
    "        else: \n",
    "            if state[0] > 0:\n",
    "                # bias the probabilities to encourage action down \n",
    "                action = np.random.choice(ACTIONS, p=[0.02,0.96,0.02])\n",
    "                action = ACTION_DOWN\n",
    "                new_state, reward = step(state, action)\n",
    "            else: # already in the bankruptcy state\n",
    "                action = ACTION_ZERO\n",
    "                new_state = [0, min(state[1] + 1, WORLD_WIDTH - 1)]\n",
    "                reward = 0    \n",
    "\n",
    "        rewards += reward\n",
    "        \n",
    "        # append a tuplet state, action, next_state, reward to the list of trajectories\n",
    "        trajectory.append((pos2idx(state), action,\n",
    "                          pos2idx(new_state),reward))\n",
    "        \n",
    "        state = new_state\n",
    "    \n",
    "        trajectory_numpy = np.array(trajectory, float)\n",
    "        \n",
    "    trajectories.append(trajectory)\n",
    "    rewards_on_traj.append(rewards)\n",
    "\n",
    "np_trajectories = np.array(trajectories, dtype='int')\n",
    "\n",
    "np.save(\"demo_trajectories\", arr=np_trajectories)\n",
    "\n",
    "\n",
    "np_trajectories = np.array(trajectories) # 3D array num_traj x num_steps x 4 \n",
    "rewards_on_traj = np.array(rewards_on_traj)\n",
    "\n",
    "print(\"len_np_trajectories\", len(np_trajectories))\n",
    "print('np_trajectories.shape = ', np_trajectories.shape)\n",
    "\n",
    "np.save(\"expert_trajectories\", arr=np_trajectories)\n",
    "np.save(\"expert_trajectory_rewards\", arr=rewards_on_traj)\n",
    "\n",
    "print(\"Done in %3.2f sec\" % (time.time() - t_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into failed and successful demonstrations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7951, 11, 4)\n",
      "(2049, 11, 4)\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "idx_failure = np.where(rewards_on_traj <= 0.0)[0]\n",
    "idx_success = np.where(rewards_on_traj >= 0.0)[0]\n",
    "\n",
    "demos_failure = np_trajectories[idx_failure, :, :]\n",
    "demos_success = np_trajectories[idx_success, :, :]\n",
    "\n",
    "rewards_failure = rewards_on_traj[idx_failure]\n",
    "rewards_success = rewards_on_traj[idx_success]\n",
    "\n",
    "print(demos_failure.shape)\n",
    "print(demos_success.shape)\n",
    "print(demos_failure.shape[0] + demos_success.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram of total rewards on failed trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, bins, patches = plt.hist(x=rewards_failure, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Total reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Trajectory rewards distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram of total rewards on successful trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, bins, patches = plt.hist(x=rewards_success, bins=12, color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Total reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Trajectory rewards distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram of total rewards on trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAcDklEQVR4nO3de5hcVZ3u8e9LwnUgAkagIUhAghAYiOQCwiAgyO2MJI4HCAclCBLFeJQZGATkaM4RHJ3xyihIFEwABYJyycyZKAEhEIcQIjSEgAwRgjQJIDdJBLn+5o+9mmwrVb0qSVdXVff7eZ56eu+1b79V3VVv70vtUkRgZmbWk/WaXYCZmbU+h4WZmWU5LMzMLMthYWZmWQ4LMzPLcliYmVmWw8J6laTZkiY1u47+QlJI2nkd1zE8rWdwGu+135GkAyQ9XBpfKunQ3lh3Wt9iSQf11vps7Q1udgHWfJJWlkY3AV4F3kzjn4qIn9S7rog4shfqmQrsHBEfW9d12erq/R1JCmBERCzpYV13AO/tjbokTQe6IuK80vp3741127pzWBgRsWn3sKSlwCcj4ubK+SQNjog3+rK2tbGudTajn+3y3Ja1Y8229nwYymqSdJCkLklfkPQU8GNJW0j6d0l/kPRCGh5WWuY2SZ8sjZ8s6aE07y8l7VCatrukOZKel/S0pHMlHQGcCxwnaaWk+9K820qaleZdIunU0nqmSvqZpCslvQScLellSe8szTM61bx+lX5WLn+SpHdIulTScklPSjpf0qA0/+OSRqfhj6VDPCPT+Ccl3ZCGx0m6U9KLaT3fk7RBabshaYqkR4BHUts/pnmXSTq5os6jJD0oaUWq6cwav7dBkr4h6VlJjwL/o2L6278jSTtLmivpj2n+a1L77Wn2+9Lv4bgafw8HSeqqKGFsqvMFST+WtFFa50mS5lXUEqmGycAJwFlpe/+Wpr99WEvShpK+k56bZWl4wzStu7YzJD2TnsNPVHt+bO04LCxnG2BLYAdgMsXfzI/T+LuBV4DvVVtQ0gSKN/6/A94F3AFclaZtBtwM/ALYFtgZuCUifgF8FbgmIjaNiL3S6q4CutK8/xP4qqRDSpsbD/wM2Bz4JnAbcGxp+seAqyPi9Rr9LC//E2AG8Eaq633AYUB3CM4FDkrDHwAeBQ4sjc9Nw28Cfw8MBd4PHAJ8pmK7E4B9gJEpKM8EPgSMACqP/V9KcVhwM2AP4Fc1+nIq8Lep7jEUz1ctXwFuArYAhgH/ChARH0jT90q/h2vSeOXfQzUnAIcD7wF2Ac6rMd/bImIaxfP+z2l7H64y2xeBfYFRwF7AuIp1bwO8A9gOOAX4vqQtctu2OkWEH368/QCWAoem4YOA14CNeph/FPBCafw2isNYALOBU0rT1gNepnijOR64t8Y6pwJXlsa3p3jj3azU9k/A9NL8t1es4zjg12l4EPAUMK6H7d1eGt+a4rzNxqW244Fb0/ApwKw0/BBFiFydxh8H9q6xndOB60vjAXywNH4Z8LXS+C5pnp3T+O+BTwFDMr/DXwGfLo0fltYzuMrv6HJgGjCsynre3natv4fU1lXx91Pe9lHA79LwScC8WtsApgPn9/D3+DvgqNK0w4GlpTpe6e5jansG2LfZr6n+8vCeheX8ISL+3D0iaRNJl6RDMS8BtwObdx+iqbAD8N10GOZF4HlAFP/5bU/x4q/HtsDzEbGi1PZ4Wk+3JyqWuZHiv/WdKP5T/2NELOhhG+XldwDWB5aXar8E2CpNnwscIGkbiiC6Bthf0nCK/2w7ASTtouIw3VPpufoqxV5Gre1uWzH+eMW8H6V48308HTp6f42+5NZTdhbF72SBiiuPTu5hXqj4e6ihctvbZuav17b8ZV8q1/1c/OU5lJeBTbFe4bCwnMrbEp9BcfXLPhExhOKwCxRvOJWeoDhssnnpsXFE/Gea9p46t7kM2DIduur2buDJWsukN7SZFIdEPg5cUWNb1ZZ/gmLPYmip7iGRrsyJ4uqgl4HPUeyRrKDYc5lM8Z/zW2k9FwO/pbiiaAjFIbnK56m83eUUIVruY7lPd0fEeIrQuiH1r5oe11Oxzqci4tSI2JZir+Ui9Xypbj23qa7c9rI0/CeKq+0ASGG7JuteRhHk1dZtDeawsDW1GcXu/ouStgS+3MO8PwDOkbQ7QDppfEya9u/ANpJOTycuN5O0T5r2NDBc0noAEfEE8J/AP0naSNKeFIeCcpf0Xk5x6ONo4Mp6OxgRyymO439T0hBJ60l6j6QDS7PNBT7LqvMTt1WMQ/FcvQSslLQrcFpm0zMpTq6PlLQJpedW0gaSTpD0jijOu7zEqsubq63nc5KGpWP2Z9faoKRjtOoChRco3rC71/s0sFOm5mqmpG1vSRGQ3ec77gN2lzQqnfSeWrFcbntXAedJepekocCXWIPfq60bh4Wtqe8AGwPPAvMpTlBXFRHXA18Hrk6HYR4AjkzTVlAcHvowxX/ljwAHp0WvTT+fk3RPGj4eGE7xn+T1wJcjYk5PhUbEr4G3gHsiYuka9BHgRGAD4EGKN9GfAR2l6XMpwuD2GuNQnKz+X8AK4IesetOsVe9siuf3V8ASVj+B/XFgaXouP01x0r6aHwK/pHhzvge4rofNjgXuUvFZm1nA5yPisTRtKjAjHYo7ttYKqvgpRdg+mh7np/79F/D/KC5seASYV7HcpRSHDl9UuqKswvnAQuB+YFHq2/lrUJetA6UTQWa9Il1y+aOIuLzZtQBI+hXw04j4UbNrMWtn/lCe9Zp06GQn4LHcvH1B0lhgb4rLYs1sHfgwlPUKSVtRHE6ay+qHF/qcpBkUhztOr7iKyszWgg9DmZlZlvcszMwsq9+esxg6dGgMHz682WWYmbWV3/zmN89GxLsq2/ttWAwfPpyFCxc2uwwzs7Yiqeon/n0YyszMshwWZmaW5bAwM7Msh4WZmWU5LMzMLMthYWZmWQ4LMzPLcliYmVmWw8LMzLIcFmY2oEyY0PSbIrclh4WZmWU5LMzMLMthYWZmWQ4LMzPLcliYmVmWw8LMzLIcFmZmluWwMDOzLIeFmZllOSzMzCzLYWFmZlkOCzMzy3JYmJlZlsPCzMyyHBZmZpblsDAzsyyHhZmZZTkszMwsy2FhZmZZDQsLSdtLulXSQ5IWS/p8at9S0hxJj6SfW5SWOUfSEkkPSzq81D5a0qI07UJJalTdZma2ukbuWbwBnBERuwH7AlMkjQTOBm6JiBHALWmcNG0isDtwBHCRpEFpXRcDk4ER6XFEA+s2M7MKDQuLiFgeEfek4RXAQ8B2wHhgRpptBjAhDY8Hro6IVyPiMWAJME5SBzAkIu6MiAAuLy1jZmZ9YHBfbETScOB9wF3A1hGxHIpAkbRVmm07YH5psa7U9noarmyvtp3JFHsgdHR00NnZ2XudMLN+Yb/9Vvq9YS00PCwkbQr8HDg9Il7q4XRDtQnRQ/vqjRHTgGkAY8aMiVGjRq1xvWbWv02dOo+zzhrV7DLaTkOvhpK0PkVQ/CQirkvNT6dDS6Sfz6T2LmD70uLDgGWpfViVdjMz6yONvBpKwKXAQxHxrdKkWcCkNDwJuLHUPlHShpJ2pDiRvSAdslohad+0zhNLy5iZWR9o5GGo/YGPA4skdaa2c4GvATMlnQL8HjgGICIWS5oJPEhxJdWUiHgzLXcaMB3YGJidHmZm1kcaFhYRMY/q5xsADqmxzAXABVXaFwJ79F51Zma2JvwJbjMzy3JYmJlZlsPCzMyyHBZmZpblsDAzsyyHhZmZZTkszMwsy2FhZmZZDgszM8tyWJiZWZbDwszMshwWZmaW5bAwM7Msh4WZmWU5LMzMLMthYWZmWQ4LMzPLcliYmVmWw8LMzLIcFmZmluWwMDOzLIeFmZllOSzMzCzLYWFmZlkOCzMzy3JYmJlZlsPCzMyyHBZmZpblsDAzsyyHhZmZZTkszMwsy2FhZmZZDgszM8tyWJiZWZbDwszMshwWZmaW5bAwM7OshoWFpMskPSPpgVLbVElPSupMj6NK086RtETSw5IOL7WPlrQoTbtQkhpVs5mZVdfIPYvpwBFV2r8dEaPS4z8AJI0EJgK7p2UukjQozX8xMBkYkR7V1mlmZg3UsLCIiNuB5+ucfTxwdUS8GhGPAUuAcZI6gCERcWdEBHA5MKEhBZuZWU2Dm7DNz0o6EVgInBERLwDbAfNL83SlttfTcGV7VZImU+yF0NHRQWdnZ+9WbmZtb7/9Vr793nDTTU9x2GHbNLegNtHXYXEx8BUg0s9vAicD1c5DRA/tVUXENGAawJgxY2LUqFHrWK6Z9TdTp87jrLNGrTZsPevTq6Ei4umIeDMi3gJ+CIxLk7qA7UuzDgOWpfZhVdpbwoQJ85pdgplZn+jTsEjnILp9BOi+UmoWMFHShpJ2pDiRvSAilgMrJO2broI6EbixL2s2M7MGHoaSdBVwEDBUUhfwZeAgSaMoDiUtBT4FEBGLJc0EHgTeAKZExJtpVadRXFm1MTA7PczMrA81LCwi4vgqzZf2MP8FwAVV2hcCe/RiaWZmtobqOgwlyW/WZmYDWL3nLH4gaYGkz0javJEFmZlZ66krLCLib4ATKK5YWijpp5I+1NDKzMysZdR9NVREPAKcB3wBOBC4UNJvJf1do4ozM7PWUO85iz0lfRt4CPgg8OGI2C0Nf7uB9ZmZWQuo92qo71F8iO7ciHiluzEilkk6ryGVmZlZy6g3LI4CXun+7IOk9YCNIuLliLiiYdWZmVlLqPecxc0UH4rrtklqMzOzAaDesNgoIlZ2j6ThTRpTkpmZtZp6w+JPkvbuHpE0Gnilh/nNzKwfqfecxenAtZK67/jaARzXkIrMzKzl1PuhvLuBXSlu6vcZYLeI+E0jCzMz6yv+uoG8NbmR4FhgeFrmfZKIiMsbUpWZmbWUusJC0hXAe4BOoPvW4d3fiW1mZv1cvXsWY4CREVHzK03NzKz/qvdqqAcAf6u5mdkAVe+exVDgQUkLgFe7GyPi6IZUZWZmLaXesJjayCLMzKy11RUWETFX0g7AiIi4WdImwKDGlmZmZq2i3luUnwr8DLgkNW0H3NCgmszMrMXUe4J7CrA/8BK8/UVIWzWqKDMzay31hsWrEfFa94ikwRSfszAzswGg3rCYK+lcYOP03dvXAv/WuLLMzKyV1BsWZwN/ABYBnwL+g+L7uM3MbACo92qotyi+VvWHjS3HzMxaUb33hnqMKucoImKnXq/IzMxazprcG6rbRsAxwJa9X46ZmbWier/P4rnS48mI+A7wwcaWZmZmraLew1B7l0bXo9jT2KwhFZmZWcup9zDUN0vDbwBLgWN7vRozM2tJ9V4NdXCjCzEzs9ZV72Gof+hpekR8q3fKMTOzVrQmV0ONBWal8Q8DtwNPNKIoMzNrLWvy5Ud7R8QKAElTgWsj4pONKszMzFpHvbf7eDfwWmn8NWB4r1djZmYtqd49iyuABZKup/gk90eAyxtWlZmZtZR6r4a6QNJs4IDU9ImIuLdxZZmZWSup9zAUwCbASxHxXaBL0o49zSzpMknPSHqg1LalpDmSHkk/tyhNO0fSEkkPSzq81D5a0qI07UJJWoOazcysF9T7tapfBr4AnJOa1geuzCw2HTiiou1s4JaIGAHcksaRNBKYCOyelrlIUvd3fF8MTAZGpEflOs3MrMHq3bP4CHA08CeAiFhG5nYfEXE78HxF83hgRhqeAUwotV8dEa9GxGPAEmCcpA5gSETcGRFBcZ5kAmZm1qfqPcH9WkSEpACQ9Fdrub2tI2I5QEQsl9T9Pd7bAfNL83WlttfTcGV7VZImU+yF0NHRQWdn51qWWZ/99lvZ8G2YWe8qv267h/1azqs3LGZKugTYXNKpwMn07hchVTsPET20VxUR04BpAGPGjIlRo0b1SnG1TJ06j7POauw2zKx3lV+33cN+LedlwyKdUL4G2BV4CXgv8KWImLMW23taUkfaq+gAnkntXcD2pfmGActS+7Aq7WZm1oey5yzSuYIbImJORPxjRJy5lkEBxe1CJqXhScCNpfaJkjZMV1mNABakQ1YrJO2bQuvE0jJmZtZH6j3BPV/S2DVZsaSrgDuB90rqknQK8DXgQ5IeAT6UxomIxcBM4EHgF8CUiHgzreo04EcUJ71/B8xekzrMzGzd1XvO4mDg05KWUlwRJYqdjj1rLRARx9eYdEiN+S8ALqjSvhDYo846zcysAXoMC0nvjojfA0f2UT1mZtaCcnsWN1DcbfZxST+PiI/2QU1mZtZicucsypeu7tTIQszMrHXlwiJqDJuZ2QCSOwy1l6SXKPYwNk7DsOoE95CGVmdmZi2hx7CIiEE9TTczs4FhTW5RbmZmA5TDwsz6vQkT5jW7hLbnsDAzsyyHhZmZZTkszMwsy2FhZmZZDgszM8tyWJiZWZbDwszMshwWZmaW5bAwM7Msh4WZmWU5LMzMLMthYWZmWQ4LMzPLcliYmVmWw8LMzLIcFmZmluWwMDOzLIeFmZllOSzMzCzLYWFmZlkOCzMzy3JYmJlZlsPCzMyyHBZmZpblsDAzsyyHhZmZZTkszMwsy2FhZmZZDgszM8tqSlhIWippkaROSQtT25aS5kh6JP3cojT/OZKWSHpY0uHNqNnMbCBr5p7FwRExKiLGpPGzgVsiYgRwSxpH0khgIrA7cARwkaRBzSjYzGygaqXDUOOBGWl4BjCh1H51RLwaEY8BS4BxfV+emdnANbhJ2w3gJkkBXBIR04CtI2I5QEQsl7RVmnc7YH5p2a7UthpJk4HJAB0dHXR2djao/MJ++61s+DbMbN2VX6vVhv1arkNE9PkD2Db93Aq4D/gA8GLFPC+kn98HPlZqvxT4aG4bo0ePjkYbP/6Ohm/DzNZd+bVabbgdX8uNqhlYGFXeU5tyGCoilqWfzwDXUxxWelpSB0D6+UyavQvYvrT4MGBZ31VrZmZ9HhaS/krSZt3DwGHAA8AsYFKabRJwYxqeBUyUtKGkHYERwIK+rdrMbGBrxjmLrYHrJXVv/6cR8QtJdwMzJZ0C/B44BiAiFkuaCTwIvAFMiYg3m1C3mdmA1edhERGPAntVaX8OOKTGMhcAFzS4NDMzq6GVLp01M7MW5bAwM7Msh4WZmWU5LMzMLMthYWZmWQ4LMzPLcliYmVmWw8LMzLIcFmZmluWwMDOzLIeFmZllOSzMzCzLYWFmZlkOCzMzy3JYmJlZlsPCzMyyHBZmZpblsDAzsyyHhZmZZTkszMwsy2FhZmZZDgszM8tyWJiZWZbDwszMshwWZmaW5bAwM7Msh4WZmWU5LMzMLMthYWZmWQ4LMzPLcliYmVmWw6JOEybMa3YJZmZN47AwM7Msh4WZmWU5LMzMLMthYWZmWQ4LMzPLapuwkHSEpIclLZF0drPrMTNrhmZdmdkWYSFpEPB94EhgJHC8pJHNrcrMrG+0wqX7bREWwDhgSUQ8GhGvAVcD4xu90Vb4BZnZwNVK70GDm11AnbYDniiNdwH7VM4kaTIwOY2ulPTwum5Yqj7cUxswFHh2XbfdYvpjn6B/9qs/9gnWsV+1XsvdwzVey42W7VO1+hpc8w7VGtslLKo9JbFaQ8Q0YFrjy+mZpIURMabZdfSm/tgn6J/96o99gv7Zr3bqU7schuoCti+NDwOWNakWM7MBp13C4m5ghKQdJW0ATARmNbkmM7MBoy0OQ0XEG5I+C/wSGARcFhGLm1xWT5p+KKwB+mOfoH/2qz/2Cfpnv9qmT4pY7dC/mZnZX2iXw1BmZtZEDgszM8tyWKwDScdIWizpLUljKqadk25N8rCkw0vtoyUtStMulJp0dXedJI2SNF9Sp6SFksaVplXtYzuQ9L9T3Ysl/XOpvW371E3SmZJC0tBSW1v2S9K/SPqtpPslXS9p89K0tuwTtOntiyLCj7V8ALsB7wVuA8aU2kcC9wEbAjsCvwMGpWkLgPdTfHZkNnBks/uR6eNN3TUCRwG35frY6g/gYOBmYMM0vlW796nUt+0pLgR5HBja7v0CDgMGp+GvA1/vB30alOrdCdgg9WNks+vKPbxnsQ4i4qGIqPYp8fHA1RHxakQ8BiwBxknqAIZExJ1R/NVcDkzou4rXSgBD0vA7WPX5lqp9bEJ9a+M04GsR8SpARDyT2tu5T92+DZzFX35otW37FRE3RcQbaXQ+xWesoI37RJNuX7SuHBaNUe32JNulR1eV9lZ2OvAvkp4AvgGck9pr9bEd7AIcIOkuSXMljU3t7dwnJB0NPBkR91VMaut+lZxMsTcO7d2ntqy9LT5n0UySbga2qTLpixFxY63FqrRFD+1N1VMfgUOAv4+In0s6FrgUOJQW7Uu3TJ8GA1sA+wJjgZmSdqLF+wTZfp1LcdhmtcWqtLVMv+p5jUn6IvAG8JPuxarM3zJ9ymjL2h0WGRFx6FosVuv2JF2s2o0utzdVT32UdDnw+TR6LfCjNNzSt2DJ9Ok04Lp0KHCBpLcobujW0n2C2v2S9NcUx+7vS9dMDAPuSRcktHS/cq8xSZOAvwUOSb8zaPE+ZbRl7T4M1RizgImSNpS0IzACWBARy4EVkvZNV0GdCNTaO2kVy4AD0/AHgUfScNU+NqG+tXEDRV+QtAvFScZnaeM+RcSiiNgqIoZHxHCKN6S9I+Ip2rhfko4AvgAcHREvlya1bZ9o09sXec9iHUj6CPCvwLuA/y+pMyIOj4jFkmYCD1LsOk+JiDfTYqcB04GNKY6/zl59zS3lVOC7kgYDfybdAj7Tx1Z3GXCZpAeA14BJ6T/Wdu5TTW3+u/oexRVPc9Ie0/yI+HQ79yna7/ZFgG/3YWZmdfBhKDMzy3JYmJlZlsPCzMyyHBZmZpblsDAzsyyHhQ1Ykt6Z7qbbKekpSU+WxjeomPd0SZvUsc7bKu9A3AokrWx2Ddbe/DkLG7Ai4jlgFICkqcDKiPhGjdlPB64EXq4xvW6SBjXyMwGSBpduvmfWK7xnYVYi6RBJ96bvHLksfUL4c8C2wK2Sbk3zXZy+32OxpP9bx3qXSvqSpHnAMZIOk3SnpHskXStpU0njJF2X5h8v6RVJG0jaSNKjqf1USXdLuk/Sz7v3diRNl/StVN/X06eD70zzfqVRz5cNHA4Ls1U2ovh0/XER8dcUe96nRcSFFLc9OTgiDk7zfjEixgB7AgdK2rOO9f85Iv6G4rs0zgMOjYi9gYXAPwD3AO9L8x4APEBxo8N9gLtS+3URMTYi9gIeAk4prX+XtM4zgO8CF0fEWOCpNXwezFbjsDBbZRDwWET8VxqfAXygxrzHSroHuBfYneLLeHKuST/3TfP/WlInMAnYIR06WiJpN4rvPPhW2v4BwB1p2T0k3SFpEXBC2na3a0uHt/YHrkrDV9RRm1mPfM7CbJU/1TNTunHdmcDYiHhB0nSKvZJ61y9gTkQcX2WeO4Ajgdcp9kCmU4TYmWn6dGBCRNwn6STgoB7q9718rNd4z8JslY2A4ZJ2TuMfB+am4RXAZml4CMUb8x8lbU3x5r4m5gP7d29H0ibp7rcAt1OcTL8zIv4AvBPYFei+0dxmwHJJ61PsWdTya4q7mZKZz6wuDguzVf4MfAK4Nh3meQv4QZo2DZgt6db0TXT3UryBX0bxxly3FAInAVdJup8iPHZNk+8CtqYIDYD7gftL3+Pwf9I8c4Df9rCZzwNTJN1N8XW4ZuvEd501M7Ms71mYmVmWw8LMzLIcFmZmluWwMDOzLIeFmZllOSzMzCzLYWFmZln/DSoJaSmkYx4UAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, bins, patches = plt.hist(x=rewards_on_traj, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Total reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Trajectory rewards distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n",
      "12\n",
      "11\n",
      "[0, 11]\n"
     ]
    }
   ],
   "source": [
    "idx = pos2idx(START)\n",
    "print(START)\n",
    "print(idx)\n",
    "print(pos2idx(GOAL))\n",
    "print(GOAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "P_a = get_transition_mat()\n",
    "print(P_a[12,:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxEnt IRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0/200\n",
      "iteration: 10/200\n",
      "iteration: 20/200\n",
      "iteration: 30/200\n",
      "iteration: 40/200\n",
      "iteration: 50/200\n",
      "iteration: 60/200\n",
      "iteration: 70/200\n",
      "iteration: 80/200\n",
      "iteration: 90/200\n",
      "iteration: 100/200\n",
      "iteration: 110/200\n",
      "iteration: 120/200\n",
      "iteration: 130/200\n",
      "iteration: 140/200\n",
      "iteration: 150/200\n",
      "iteration: 160/200\n",
      "iteration: 170/200\n",
      "iteration: 180/200\n",
      "iteration: 190/200\n"
     ]
    }
   ],
   "source": [
    "n_actions = len(ACTIONS)\n",
    "n_states = WORLD_HEIGHT * WORLD_WIDTH\n",
    "\n",
    "lr = 0.02 # learning rate\n",
    "lr_Q = 0.1 \n",
    "beta = 1.0 \n",
    "        \n",
    "n_iters = 200 \n",
    "\n",
    "GAMMA = 0.95\n",
    "gamma = GAMMA\n",
    "\n",
    "P_a = get_transition_mat()\n",
    "\n",
    "# The one-hot basis is formed by the columns or rows of a unit matrix \n",
    "feature_matrix = np.eye((n_states)) \n",
    "\n",
    "# MaxEnt IRL\n",
    "irl_rewards =  maxent_irl(feature_matrix, P_a, gamma, beta, lr, lr_Q, demos_success, n_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results from MaxEnt IRL and compare with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth rewards (shape n_state x n_actions)\n",
    "gt_rewards = get_true_rewards()\n",
    "\n",
    "# ground truth values\n",
    "values_gt, _ = value_iteration(P_a, gt_rewards, GAMMA, error=0.01, deterministic=True)\n",
    "\n",
    "# the value function recovered from IRL\n",
    "values_irl, _ = value_iteration(P_a, irl_rewards, GAMMA, error=0.01, deterministic=True)\n",
    "\n",
    "# plots\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "H = WORLD_HEIGHT\n",
    "W = WORLD_WIDTH\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_UP), \n",
    "                    'Rewards ACTION_UP - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_UP),  \n",
    "                    'Rewards ACTION_UP - Recovered', block=False)\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_DOWN), \n",
    "                    'Rewards ACTION_DOWN - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_DOWN), \n",
    "                    'Rewards ACTION_DOWN - Recovered', block=False)\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 5)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_ZERO), \n",
    "                    'Rewards ACTION_ZERO - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_ZERO), \n",
    "                    'Rewards ACTION_ZERO - Recovered', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 7)\n",
    "heatmap2d(get_values_on_rectangle(values_gt), 'Value Map - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 8)\n",
    "heatmap2d(get_values_on_rectangle(values_irl), 'Value Map - Recovered', block=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run IRLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = len(ACTIONS)\n",
    "n_states = WORLD_HEIGHT * WORLD_WIDTH\n",
    "\n",
    "lr = 0.02 # learning rate\n",
    "lr_Q = 0.5 \n",
    "beta = 1.0 \n",
    "\n",
    "lam = 0.5 \n",
    "lam_min = 1.0 \n",
    "alpha_lam = 0.2\n",
    "\n",
    "n_iters = 200 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "gamma = GAMMA\n",
    "\n",
    "P_a = get_transition_mat()\n",
    "\n",
    "# The one-hot basis is formed by the columns or rows of a unit matrix \n",
    "feature_matrix = np.eye((n_states)) \n",
    "\n",
    "# take all successful demos, and the same number of randomly sampled failed trajectories\n",
    "\n",
    "num_success = demos_success.shape[0]\n",
    "num_failures = demos_failure.shape[0]\n",
    "rand_idx = np.random.choice(np.arange(num_failures), num_success)\n",
    "\n",
    "demos_failure_subset = demos_failure[rand_idx,:,:]\n",
    "\n",
    "\n",
    "# IRL from failure\n",
    "irlf_rewards, policy, theta_s, theta_f = irl_from_failure(feature_matrix, P_a, gamma, \n",
    "                                        beta,\n",
    "                                        demos_success, \n",
    "                                        demos_failure_subset,\n",
    "                                        lr,\n",
    "                                        lr_Q,\n",
    "                                        alpha_lam,\n",
    "                                        lam,\n",
    "                                        lam_min, \n",
    "                                        n_iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test: compare mean values of parameters defining the rewards for 'success'- and 'failure'- associated features \n",
    "print(np.mean(theta_s))\n",
    "print(np.mean(theta_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results from IRLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth rewards (shape n_state x n_actions)\n",
    "gt_rewards = get_true_rewards()\n",
    "\n",
    "# ground truth values\n",
    "values_gt, _ = value_iteration(P_a, gt_rewards, GAMMA, error=0.01, deterministic=True)\n",
    "\n",
    "# the value function recovered from IRL\n",
    "values_irlf, _ = value_iteration(P_a, irlf_rewards, GAMMA, error=0.01, deterministic=True)\n",
    "\n",
    "# plots\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "H = WORLD_HEIGHT\n",
    "W = WORLD_WIDTH\n",
    "\n",
    "# invert the y-axis\n",
    "plt.ylim(plt.ylim()[::-1])\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_UP), \n",
    "                    'Rewards ACTION_UP - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "heatmap2d(get_rewards_on_rectangle(irlf_rewards, ACTION_UP),  \n",
    "                    'Rewards ACTION_UP - Recovered', block=False)\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_DOWN), \n",
    "                    'Rewards ACTION_DOWN - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "heatmap2d(get_rewards_on_rectangle(irlf_rewards, ACTION_DOWN), \n",
    "                    'Rewards ACTION_DOWN - Recovered', block=False)\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 5)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_ZERO), \n",
    "                    'Rewards ACTION_ZERO - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "heatmap2d(get_rewards_on_rectangle(irlf_rewards, ACTION_ZERO), \n",
    "                    'Rewards ACTION_ZERO - Recovered', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 7)\n",
    "heatmap2d(get_values_on_rectangle(values_gt), 'Value Map - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 8)\n",
    "heatmap2d(get_values_on_rectangle(values_irlf), 'Value Map - Recovered', block=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-REX with a tabular representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce sorted trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_reward_sorted = np.argsort(rewards_on_traj)\n",
    "sorted_trajs = np_trajectories[np.argsort(rewards_on_traj),:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(demonstrations, num_trajs, num_snippets, min_snippet_length, max_snippet_length):\n",
    "    \"\"\"\n",
    "    Function to produce pairs of snippets of trajectories from demonstrations\n",
    "    \n",
    "    Returns:\n",
    "    training_obs: list of tuples containing pairs of trajectory snippets\n",
    "    training_labels: list of labels for these tuples (1 if the second trajectory is ranked higher, 0 otherwise)\n",
    "    \n",
    "    \"\"\"\n",
    "    # collect training data\n",
    "    max_traj_length = 0\n",
    "    training_obs = []\n",
    "    training_labels = []\n",
    "    num_demos = len(demonstrations)\n",
    "\n",
    "    # fixed size snippets with progress prior\n",
    "    for n in range(num_snippets):\n",
    "        ti = 0\n",
    "        tj = 0\n",
    "        # only add trajectories that are different returns\n",
    "        while(ti == tj):\n",
    "            #pick two random demonstrations\n",
    "            ti = np.random.randint(num_demos)\n",
    "            tj = np.random.randint(num_demos)\n",
    "        \n",
    "        # create random snippets\n",
    "        # find min length of both demos to ensure we can pick a demo no earlier than that chosen in worse preferred demo\n",
    "        min_length = min(len(demonstrations[ti]), len(demonstrations[tj]))\n",
    "        rand_length = np.random.randint(min_snippet_length, max_snippet_length)\n",
    "        if ti < tj: # pick tj snippet to be later than ti\n",
    "            ti_start = np.random.randint(min_length - rand_length + 1)\n",
    "            tj_start = np.random.randint(ti_start, len(demonstrations[tj]) - rand_length + 1)\n",
    "        else: # ti is better so pick later snippet in ti\n",
    "            tj_start = np.random.randint(min_length - rand_length + 1)\n",
    "            ti_start = np.random.randint(tj_start, len(demonstrations[ti]) - rand_length + 1)\n",
    "        \n",
    "        # override this part -IH        \n",
    "        traj_i = demonstrations[ti,ti_start:ti_start+rand_length, 0:2]  # 0:2 retains only the state and action\n",
    "        traj_j = demonstrations[tj,tj_start:tj_start+rand_length, 0:2]\n",
    "                \n",
    "        max_traj_length = max(max_traj_length, len(traj_i), len(traj_j))\n",
    "        if ti > tj:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "        training_obs.append((traj_i, traj_j))\n",
    "        training_labels.append(label)\n",
    "\n",
    "    print(\"maximum traj length\", max_traj_length)\n",
    "    return training_obs, training_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of function\n",
    "# (not needed below, the function create_training_data will be called separately below)\n",
    "demonstrations = sorted_trajs[0:10,:,:]\n",
    "\n",
    "num_trajs = 10\n",
    "num_snippets = 12\n",
    "min_snippet_length = 4\n",
    "max_snippet_length = 5\n",
    "\n",
    "training_obs, training_labels = create_training_data(demonstrations, \n",
    "                                                     num_trajs, \n",
    "                                                     num_snippets,\n",
    "                                                     min_snippet_length, \n",
    "                                                     max_snippet_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-REX (a tabular version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T_REX_tabular(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, outputSize, device):\n",
    "        \n",
    "        super(T_REX_tabular, self).__init__()\n",
    "        \n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.device = device\n",
    "        \n",
    "        # try initialization with zeros\n",
    "        self.theta = nn.Parameter(torch.zeros((n_actions,n_states), \n",
    "                                              requires_grad=True, \n",
    "                                              dtype=torch.float, device=device))\n",
    "        \n",
    "\n",
    "    def cum_return(self, traj):\n",
    "        '''calculate cumulative return of trajectory'''\n",
    "\n",
    "        sum_rewards = 0\n",
    "        sum_abs_rewards = 0\n",
    "        \n",
    "        # trajectory consists of sequences (state, action)\n",
    "        # use these values to compute rewards as functions of theta\n",
    "        \n",
    "        # iterate over all rows \n",
    "        for row in traj.split(1):\n",
    "            state, action = row[0,0], row[0,1]\n",
    "            \n",
    "            # the reward for this state-action combination is the corresponding element of \n",
    "            # array of theta\n",
    "            r = self.theta[int(action.detach().numpy()), int(state.detach().numpy())]\n",
    "            \n",
    "            # add this reward to cumulative values\n",
    "            sum_rewards = sum_rewards + r\n",
    "            sum_abs_rewards = sum_abs_rewards + torch.abs(r)\n",
    "                \n",
    "        return sum_rewards, sum_abs_rewards\n",
    "\n",
    "\n",
    "    def forward(self, traj_i, traj_j):\n",
    "        '''compute cumulative return for each trajectory and return logits'''\n",
    "        cum_r_i, abs_r_i = self.cum_return(traj_i)\n",
    "        cum_r_j, abs_r_j = self.cum_return(traj_j)\n",
    "        \n",
    "        return torch.cat((cum_r_i.unsqueeze(0), cum_r_j.unsqueeze(0)),0), abs_r_i + abs_r_j\n",
    "\n",
    "\n",
    "# Train the network\n",
    "def learn_reward(reward_network, \n",
    "                 optimizer, \n",
    "                 training_inputs, \n",
    "                 training_outputs, \n",
    "                 num_iter, l1_reg, \n",
    "                 checkpoint_dir):\n",
    "    \n",
    "    # check if gpu available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    \n",
    "    loss_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    cum_loss = 0.0\n",
    "    training_data = list(zip(training_inputs, training_outputs))\n",
    "    for epoch in range(num_iter):\n",
    "        np.random.shuffle(training_data)\n",
    "        training_obs, training_labels = zip(*training_data)\n",
    "        for i in range(len(training_labels)):\n",
    "            \n",
    "            # take sub-trajectories \n",
    "            traj_i, traj_j = training_obs[i]\n",
    "            labels = np.array([training_labels[i]])\n",
    "            traj_i = np.array(traj_i)\n",
    "            traj_j = np.array(traj_j)\n",
    "            \n",
    "            # convert to Pytorch tensors \n",
    "            traj_i = torch.from_numpy(traj_i).float().to(device)\n",
    "            traj_j = torch.from_numpy(traj_j).float().to(device)\n",
    "            labels = torch.from_numpy(labels).to(device)\n",
    "\n",
    "            # zero out gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward path:\n",
    "            # compute cumulative rewards and model outputs \n",
    "            # for current values of parameters\n",
    "            outputs, abs_rewards = reward_network.forward(traj_i, traj_j)\n",
    "            outputs = outputs.unsqueeze(0)\n",
    "                      \n",
    "            # the loss is cross-entropy with L1 regularization    \n",
    "            loss = loss_criterion(outputs, labels) + l1_reg * abs_rewards\n",
    "            \n",
    "            # optimization (backward path)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print stats to see if learning\n",
    "            item_loss = loss.item()\n",
    "            cum_loss += item_loss\n",
    "            if i % 100 == 99:\n",
    "                #print(i)\n",
    "                print(\"epoch {}:{} loss {}\".format(epoch,i, cum_loss))\n",
    "                print(abs_rewards)\n",
    "                cum_loss = 0.0\n",
    "                print(\"check pointing\")\n",
    "                torch.save(reward_net.state_dict(), checkpoint_dir)\n",
    "    print(\"finished training\")\n",
    "\n",
    "\n",
    "# additional functions\n",
    "def calc_accuracy(reward_network, training_inputs, training_outputs):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loss_criterion = nn.CrossEntropyLoss()\n",
    "    num_correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(training_inputs)):\n",
    "            label = training_outputs[i]\n",
    "            traj_i, traj_j = training_inputs[i]\n",
    "            traj_i = np.array(traj_i)\n",
    "            traj_j = np.array(traj_j)\n",
    "            traj_i = torch.from_numpy(traj_i).float().to(device)\n",
    "            traj_j = torch.from_numpy(traj_j).float().to(device)\n",
    "\n",
    "            #forward to get logits\n",
    "            outputs, abs_return = reward_network.forward(traj_i, traj_j)\n",
    "            _, pred_label = torch.max(outputs,0)\n",
    "            if pred_label.item() == label:\n",
    "                num_correct += 1.\n",
    "    return num_correct / len(training_inputs)\n",
    "\n",
    "def predict_reward_sequence(net, traj):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    rewards_from_obs = []\n",
    "    with torch.no_grad():\n",
    "        for s in traj:\n",
    "            r = net.cum_return(torch.from_numpy(np.array([s])).float().to(device))[0].item()\n",
    "            rewards_from_obs.append(r)\n",
    "    return rewards_from_obs\n",
    "\n",
    "def predict_traj_return(net, traj):\n",
    "    return sum(predict_reward_sequence(net, traj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data for T-REX-tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = \"beamrider\"\n",
    "reward_model_path = \"./learned_models/FCW.params\"\n",
    "seed = 42\n",
    "models_dir = \".\"\n",
    "num_trajs = 10000\n",
    "num_snippets = 10000\n",
    "\n",
    "min_snippet_length = 4\n",
    "max_snippet_length = 5\n",
    "\n",
    "lr = 0.01 # 0.05 # 0.00005\n",
    "weight_decay = 0.0\n",
    "num_iter = 5 # num times to iterate through training data\n",
    "l1_reg=0.0\n",
    "stochastic = True\n",
    "\n",
    "n_states = WORLD_HEIGHT * WORLD_WIDTH\n",
    "n_actions = 3\n",
    "\n",
    "outputDim = 1  # takes variable 'y'\n",
    "epochs = 300\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "idx_reward_sorted = np.argsort(rewards_on_traj)\n",
    "sorted_trajs = np_trajectories[np.argsort(rewards_on_traj),:,:]\n",
    "sorted_traj_rewards = rewards_on_traj[idx_reward_sorted]\n",
    "\n",
    "demonstrations = sorted_trajs[0:num_trajs,:,:]\n",
    "    \n",
    "training_obs, training_labels = create_training_data(demonstrations, num_trajs, num_snippets, \n",
    "                                                     min_snippet_length, max_snippet_length)\n",
    "\n",
    "\n",
    "print(\"num training_obs\", len(training_obs))\n",
    "print(\"num_labels\", len(training_labels))\n",
    "   \n",
    "# Now we create a reward network and optimize it using the training data.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the T-REX model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "reward_net = T_REX_tabular(n_states, n_actions, outputDim, device)\n",
    "reward_net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(reward_net.parameters(),  lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "learn_reward(reward_net, optimizer, training_obs, training_labels, num_iter, l1_reg, reward_model_path)\n",
    "\n",
    "#save reward network\n",
    "torch.save(reward_net.state_dict(), reward_model_path)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from T-REX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_rewards = reward_net.theta.detach().numpy().T\n",
    "\n",
    "# plots\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "H = WORLD_HEIGHT\n",
    "W = WORLD_WIDTH\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_UP), \n",
    "                    'Rewards ACTION_UP - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_UP),  \n",
    "                    'Rewards ACTION_UP - Recovered', block=False)\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_DOWN), \n",
    "                    'Rewards ACTION_DOWN - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_DOWN), \n",
    "                    'Rewards ACTION_DOWN - Recovered', block=False)\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 5)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_ZERO), \n",
    "                    'Rewards ACTION_ZERO - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_ZERO), \n",
    "                    'Rewards ACTION_ZERO - Recovered', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 7)\n",
    "heatmap2d(get_values_on_rectangle(values_gt), 'Value Map - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 8)\n",
    "heatmap2d(get_values_on_rectangle(values_irl), 'Value Map - Recovered', block=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_rewards = reward_net.theta.detach().numpy().T\n",
    "\n",
    "# plots\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "H = WORLD_HEIGHT\n",
    "W = WORLD_WIDTH\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_UP), \n",
    "                    'Rewards ACTION_UP - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_UP),  \n",
    "                    'Rewards ACTION_UP - Recovered', block=False)\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_DOWN), \n",
    "                    'Rewards ACTION_DOWN - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_DOWN), \n",
    "                    'Rewards ACTION_DOWN - Recovered', block=False)\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 5)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_ZERO), \n",
    "                    'Rewards ACTION_ZERO - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_ZERO), \n",
    "                    'Rewards ACTION_ZERO - Recovered', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 7)\n",
    "heatmap2d(get_values_on_rectangle(values_gt), 'Value Map - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 8)\n",
    "heatmap2d(get_values_on_rectangle(values_irl), 'Value Map - Recovered', block=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results for the following choice:\n",
    "irl_rewards = reward_net.theta.detach().numpy().T\n",
    "\n",
    "# plots\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "H = WORLD_HEIGHT\n",
    "W = WORLD_WIDTH\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_UP), \n",
    "                    'Rewards ACTION_UP - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_UP),  \n",
    "                    'Rewards ACTION_UP - Recovered', block=False)\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_DOWN), \n",
    "                    'Rewards ACTION_DOWN - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_DOWN), \n",
    "                    'Rewards ACTION_DOWN - Recovered', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 5)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_ZERO), \n",
    "                    'Rewards ACTION_ZERO - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_ZERO), \n",
    "                    'Rewards ACTION_ZERO - Recovered', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 7)\n",
    "heatmap2d(get_values_on_rectangle(values_gt), 'Value Map - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 8)\n",
    "heatmap2d(get_values_on_rectangle(values_irl), 'Value Map - Recovered', block=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_rewards = reward_net.theta.detach().numpy().T\n",
    "\n",
    "# plots\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "H = WORLD_HEIGHT\n",
    "W = WORLD_WIDTH\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 1)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_UP), \n",
    "                    'Rewards ACTION_UP - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_UP),  \n",
    "                    'Rewards ACTION_UP - Recovered', block=False)\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_DOWN), \n",
    "                    'Rewards ACTION_DOWN - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_DOWN), \n",
    "                    'Rewards ACTION_DOWN - Recovered', block=False)\n",
    "\n",
    "\n",
    "plt.subplot(4, 2, 5)\n",
    "heatmap2d(get_rewards_on_rectangle(gt_rewards, ACTION_ZERO), \n",
    "                    'Rewards ACTION_ZERO - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "heatmap2d(get_rewards_on_rectangle(irl_rewards, ACTION_ZERO), \n",
    "                    'Rewards ACTION_ZERO - Recovered', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 7)\n",
    "heatmap2d(get_values_on_rectangle(values_gt), 'Value Map - Ground Truth', block=False)\n",
    "\n",
    "plt.subplot(4, 2, 8)\n",
    "heatmap2d(get_values_on_rectangle(values_irl), 'Value Map - Recovered', block=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
